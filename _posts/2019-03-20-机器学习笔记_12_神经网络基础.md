---
title: 机器学习笔记_12_神经网络基础
categories:
- ML
tags:
- tensorflow
updated: 2019-03-21
---



 

笔记来源：培训视频       tensorflow_神经网络基础

**感知机：**

有n个输入数据，通过权重和各数据之间的计算和，比较激活函数结果，得到输出。

应用：很容易解决与、或问题。

<img src="{{ site.url }}/assets//blog_images/ML/神经网络基础_01.png" />

单个感知机无法解决的时候，可以增加感知机。



**神经网络的结构：**

感知机--》神经元--》多个--》神经网络

**神经网络特点：**

<img src="{{ site.url }}/assets//blog_images/ML/神经网络基础_02.png" />

**神经网络的组成：**

<img src="{{ site.url }}/assets//blog_images/ML/神经网络基础_03.png" />

逻辑回归： sigmoid-->某一个类别的概率     二分类

神经网络： 多分类   

​	某一个样本-->得出属于全部类别的每一个概率

神经网络的API模块：

<img src="{{ site.url }}/assets//blog_images/ML/神经网络基础_04.png" />

**SoftMax回归：**

<img src="{{ site.url }}/assets//blog_images/ML/神经网络基础_05.png" />

SoftMax回归： 

​	1.计算概率

​	2.所有类别的概率值相加都等于1

算法				策略				优化

线性回归				均方误差				梯度下降

逻辑回归				对数似然最大值 		梯度下降

神经网络				交叉熵损失			反向传播

**交叉熵损失：**

​	<img src="{{ site.url }}/assets//blog_images/ML/神经网络基础_06.png" />

一个样本就有一个交叉熵损失

正向传播：输出经过一层层的计算得出损失

反向传播：从损失计算开始，梯度下降更新权重

<img src="{{ site.url }}/assets//blog_images/ML/神经网络基础_07.png" />

<img src="{{ site.url }}/assets//blog_images/ML/神经网络基础_08.png" />

<img src="{{ site.url }}/assets//blog_images/ML/神经网络基础_09.png" />

<img src="{{ site.url }}/assets//blog_images/ML/神经网络基础_10.png" />

Mnist数据集的识别（使用的单层全连接层）：

获取数据：

<img src="{{ site.url }}/assets//blog_images/ML/神经网络基础_11.png" />

准确性计算：

<img src="{{ site.url }}/assets//blog_images/ML/神经网络基础_12.png" />

流程：

<img src="{{ site.url }}/assets//blog_images/ML/神经网络基础_13.png" />

```python
# coding: utf-8
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data


def full_connected():
    # 获取真实的数据
    mnist = input_data.read_data_sets("G:/PGD_SGA/mnist/mnist_challenge-master/"
                                      "mnist_challenge-master/MNIST_data/",one_hot=True)
    # 1.建立数据的占位符， x[None,784]  y_true [None,10]
    with tf.variable_scope("data"):
        x = tf.placeholder(tf.float32, [None, 784])
        y_true = tf.placeholder(tf.int32, [None,10])
    # 2.建立一个全连接层的神经网络 w [784,10] b[10]
    with tf.variable_scope("fc_model"):
        # 随机初始化权重和偏置
        weight = tf.Variable(tf.random_normal([784,10], mean=0.0, stddev=1.0), name="w")
        # 随机初始化
        bias = tf.Variable(tf.constant(0.0, shape=[10]))
        # 预测None个样本的输出结果[None, 784] * [784,10] + [10] = [None,10]
        y_predict = tf.matmul(x,weight) + bias
        # 3.求出所有样本的损失，然后求平均值
    with tf.variable_scope("soft_cross"):
        # 求平均交叉熵损失
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_predict))
    # 4.梯度下降求出损失
    with tf.variable_scope("optimizer"):
        train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
    # 5.计算准确率
    with tf.variable_scope("accuracy"):
        #
        equal_list = tf.equal(tf.arg_max(y_true, 1), tf.arg_max(y_predict, 1))
        # equal_list None个样本 [1,0,1,0,0,1,.....]
        accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))
    # 收集变量 单个数字收集
    tf.summary.scalar("losses", loss)
    tf.summary.scalar("acc", accuracy)
    # 高纬度变量收集
    tf.summary.histogram("weights", weight)
    tf.summary.histogram("biases", bias)
    # 定义一个初始化变量的op
    init_op = tf.global_variables_initializer()
    # 定义一个合并变量的OP
    merged = tf.summary.merge_all()

    # 开启会话训练
    with tf.Session() as sess:
        # 初始化变量
        sess.run(init_op)
        # 建立events文件，然后写入
        filewriter = tf.summary.FileWriter("./test/", graph=sess.graph)
        # 迭代步数去训练，更新参数预测
        for i in range(2000):
            # 取出真实存在的特征值和目标值
            mnist_x,mnist_y = mnist.train.next_batch(50)
            sess.run(train_op, feed_dict={x: mnist_x, y_true: mnist_y})
            # 写入每步训练的值
            summary = sess.run(merged, feed_dict={x: mnist_x, y_true: mnist_y})
            filewriter.add_summary(summary, i)

            print("训练第%d步，准确率为:%f"%(i,sess.run(accuracy, feed_dict=
                                                {x: mnist_x, y_true:mnist_y })))

    return None


if __name__ == "__main__":
    full_connected()
```

<img src="{{ site.url }}/assets//blog_images/ML/神经网络基础_14png" />

<img src="{{ site.url }}/assets//blog_images/ML/神经网络基础_15.png" />

<img src="{{ site.url }}/assets//blog_images/ML/神经网络基础_16.png" />

<img src="{{ site.url }}/assets//blog_images/ML/神经网络基础_17.png" />

<img src="{{ site.url }}/assets//blog_images/ML/神经网络基础_18.png" />





































