---
title: 机器学习笔记_01
categories:
- ML
tags:
- tensorflow
updated: 2019-03-08
---



 

视频来源：中国慕课网 人工智能实践：tensorflow笔记

定义：1.机器学习是一种统计学方法，计算机利用已有数据，得出某种模型，再利用此模型预测出结果

特点：随经验的增加，效果会变好。 

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_01.png" />

人工智能，就是用机器模拟人的意识和思维。

 机器学习，则是实现人工智能的一种方法，是人工智能的子集。

 深度学习就是深层次神经网络，是机器学习的一种实现方法，是机器学习的子集。 

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_02.png" />



Promise me，never look back ,never settle down ,It'll stop raining, It'll will be sunny, Nothing's

gonna be so bad. You hava to force yourself to be good, and then,be proud of your life.

基于tensorflow的NN：用张量表示数据，用计算图搭建神经网络，用会话执行计算图，优化线上的权重（参数），得到模型。

张量：张量就是多维数组（列表），用“阶”表示张量的维度。 

0 阶张量称作标量，表示一个单独的数；

 举例 S=123 1 阶张量称作向量，表示一个一维数组；

 举例 V=[1,2,3] 2 阶张量称作矩阵，表示一个二维数组，它可以有 i 行 j 列个元素，每个元素可 以用行号和列号共同索引到； 

举例 m=[[1, 2, 3], [4, 5, 6], [7, 8, 9]] 判断张量是几阶的，就通过张量右边的方括号数，0 个是 0 阶，n 个是 n 阶，张 量可以表示 0 阶到 n 阶数组（列表）； 

举例 t=[ [ [… ] ] ]为 3 阶。 

数据类型：Tensorflow 的数据类型有 tf.float32、tf.int32 等。  

import tensorflow as tf  #引入模块 

a = tf.constant([1.0, 2.0]) #定义一个张量等于[1.0,2.0] 

b = tf.constant([3.0, 4.0]) #定义一个张量等于[3.0,4.0] 

result = a+b #实现 a 加 b 的加法 print result #打印出结果

 可以打印出这样一句话：Tensor(“add:0”, shape=(2, ), dtype=float32)， 意思为 result 是一个名称为 add:0 的张量，shape=(2,)表示一维数组长度为 2， dtype=float32 表示数据类型为浮点型。 

**计算图******（Graph）：搭建神经网络的计算过程，是承载一个或多个计算节点的一 张图，只搭建网络，不运算。**** 

**会话（Session）：执行计算图中的节点运算。** 

**一、神经网络的参数**

 √神经网络的参数：是指神经元线上的权重 w，用变量表示，一般会先随机生成 这些参数。生成参数的方法是让w等于tf.Variable，把生成的方式写在括号里。 神经网络中常用的生成随机数/数组的函数有：

 tf.random_normal()   生成正态分布随机数 

tf.truncated_normal() 生成去掉过大偏离点的正态分布随机数

 tf.random_uniform() 生成均匀分布随机数 

tf.zeros 表示生成全 0 数组 tf.ones 表示生成全 1 数组

 tf.fill 表示生成全定值数组

 tf.constant 表示生成直接给定值的数组 

举例 

① w=tf.Variable(tf.random_normal([2,3],stddev=2, mean=0, seed=1))，表 示生成正态分布随机数，形状两行三列，标准差是 2，均值是 0，随机种子是 1。

 ② w=tf.Variable(tf.Truncated_normal([2,3],stddev=2, mean=0, seed=1))， 表示去掉偏离过大的正态分布，也就是如果随机出来的数据偏离平均值超过两个 标准差，这个数据将重新生成。

 ③ w=random_uniform(shape=7,minval=0,maxval=1,dtype=tf.int32，seed=1), 表示从一个均匀分布[minval maxval)中随机采样，注意定义域是左闭右开，即 包含 minval，不包含 maxval。

 ④ 除了生成随机数，还可以生成常量。tf.zeros([3,2],int32)表示生成 [[0,0],[0,0],[0,0]]；tf.ones([3,2],int32)表示生成[[1,1],[1,1],[1,1]； tf.fill([3,2],6)表示生成[[6,6],[6,6],[6,6]]；tf.constant([3,2,1])表示 生成[3,2,1]。 

注意：①随机种子如果去掉每次生成的随机数将不一致。

 ②如果没有特殊要求标准差、均值、随机种子是可以不写的。 

**二、神经网络的搭建** 

当我们知道张量、计算图、会话和参数后，我们可以讨论神经网络的实现过程了

。 √神经网络的实现过程：

 1、准备数据集，提取特征，作为输入喂给神经网络（Neural Network，NN）

 2、搭建 NN 结构，从输入到输出（先搭建计算图，再用会话执行） （ NN 前向传播算法 计算输出）

 3、大量特征数据喂给 NN，迭代优化 NN 参数 （ NN 反向传播算法 优化参数训练模型）

 4、使用训练好的模型预测和分类

 由此可见，基于神经网络的机器学习主要分为两个过程，即训练过程和使用过程。 训练过程是第一步、第二步、第三步的循环迭代，使用过程是第四步，一旦参数 优化完成就可以固定这些参数，实现特定应用了。 很多实际应用中，我们会先使用现有的成熟网络结构，喂入新的数据，训练相应 模型，判断是否能对喂入的从未见过的新数据作出正确响应，再适当更改网络结 构，反复迭代，让机器自动训练参数找出最优结构和参数，以固定专用模型。 

**三、前向传播** 

√前向传播就是搭建模型的计算过程，让模型具有推理能力，可以针对一组输入 给出相应的输出。  

√前向传播过程的 tensorflow 描述：

 √变量初始化、计算图节点运算都要用会话（with 结构）实现 

with tf.Session() as sess: 

​	sess.run() 

√变量初始化：在 sess.run 函数中用 tf.global_variables_initializer()汇 总所有待优化变量。 

 init_op = tf.global_variables_initializer() 

sess.run(init_op)

 √计算图节点运算：在 sess.run 函数中写入待运算的节点 

sess.run(y)

 √用 tf.placeholder 占位，在 sess.run 函数中用 feed_dict 喂数据

 喂一组数据：

 x = tf.placeholder(tf.float32, shape=(1, 2)) 

sess.run(y, feed_dict={x: [[0.5,0.6]]}) 

喂多组数据：

 x = tf.placeholder(tf.float32, shape=(None, 2)) 

sess.run(y, feed_dict={x: [[0.1,0.2],[0.2,0.3],[0.3,0.4],[0.4,0.5]]})  

**一、反向传播** 

**√反向传播**：训练模型参数，在所有参数上用梯度下降，使 NN 模型在训练数据 上的损失函数最小。

 **√损失函数（loss）**：计算得到的预测值 y 与已知答案 y_的差距。

 损失函数的计算有很多方法，均方误差 MSE 是比较常用的方法之一。

 **√均方误差 MSE**：求前向传播计算结果与已知答案之差的平方再求平均。  

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_03.png" />

用 tensorflow 函数表示为：

 loss_mse = tf.reduce_mean(tf.square(y_ - y)) 

√反向传播训练方法：以减小 loss 值为优化目标，有梯度下降、momentum 优化 器、adam 优化器等优化方法。

 这三种优化方法用 tensorflow 的函数可以表示为： train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) train_step=tf.train.MomentumOptimizer(learning_rate, momentum).minimize(loss) train_step=tf.train.AdamOptimizer(learning_rate).minimize(loss)  

三种优化方法区别如下：

 ①tf.train.GradientDescentOptimizer()使用随机梯度下降算法，使参数沿着 梯度的反方向，即总损失减小的方向移动，实现更新参数。 

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_04.png" />

其中，𝐽(𝜃)为损失函数，𝜃为参数，𝛼为学习率。 

②tf.train.MomentumOptimizer()在更新参数时，利用了超参数，参数更新公式 是 

​				𝑑𝑖 = 𝛽𝑑𝑖−1 + 𝑔(𝜃𝑖−1 ) 

​				𝜃𝑖 = 𝜃𝑖−1 − 𝛼𝑑𝑖

 其中，𝛼为学习率，超参数为𝛽，𝜃为参数，𝑔(𝜃𝑖−1 )为损失函数的梯度。 ③tf.train.AdamOptimizer()是利用自适应学习率的优化算法，Adam 算法和随 机梯度下降算法不同。随机梯度下降算法保持单一的学习率更新所有的参数，学 习率在训练过程中并不会改变。而 Adam 算法通过计算梯度的一阶矩估计和二 阶矩估计而为不同的参数设计独立的自适应性学习率。 

√**学习率：决定每次参数更新的幅度。**

 优化器中都需要一个叫做学习率的参数，使用时，如果学习率选择过大会出现震 荡不收敛的情况，如果学习率选择过小，会出现收敛速度慢的情况。我们可以选 个比较小的值填入，比如 0.01、0.001。 

进阶：

反向传播参数更新推导过程 符号说明：

 𝑧 𝑙表示第l层隐藏层和输出层的输入值；

 𝑎 𝑙表示第l层隐藏层和输出层的输出值； 

f(z)表示激活函数； 

最后的输出层为第 L 层。 

推导过程：

 前向传播第 L 层输出：𝑎 𝑙 = 𝑓(𝑧 𝑙 ) = 𝑓(𝑤 𝑙𝑎 𝑙−1 + 𝑏 𝑙 ) 



<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_05.png" />

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_06.png" />



二、搭建神经网络的八股
 我们最后梳理出神经网络搭建的八股，神经网络的搭建课分四步完成：准备工作、 前向传播、反向传播和循环迭代。

 √0.导入模块，生成模拟数据集；

 import 常量定义 

生成数据集 

√1.前向传播：定义输入、参数和输出

 x=     y_=   

 w1= 	w2= 

a=	 y= 

√2. 反向传播：定义损失函数、反向传播方法 

loss=

 train_step=

 √3. 生成会话，训练 STEPS 轮

 with tf.session() as sess 

​	Init_op=tf. global_variables_initializer()

​	 sess_run(init_op) 

​	STEPS=3000

​	 for i in range(STEPS): 

​		start= 

​		end= 

​		sess.run(train_step, feed_dict:)  
