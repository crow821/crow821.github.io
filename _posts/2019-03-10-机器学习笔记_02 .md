---
title: 机器学习笔记_02
categories:
- ML
tags:
- tensorflow
updated: 2019-03-10
---



 

视频来源：中国慕课网 人工智能实践：tensorflow笔记_神经网络优化

 √神经元模型：用数学公式表示为：𝐟(∑𝒊𝒙𝒊𝒘𝒊 + 𝐛)，f 为激活函数。神经网络是以神经元为基本单 元构成的。 

√激活函数：引入非线性激活因素，提高模型的表达力。

 常用的激活函数有 relu、sigmoid、tanh 等。 

① 激活函数 relu: 在 Tensorflow 中，用 tf.nn.relu()表示  

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_07.png" />

② 激活函数 sigmoid：在 Tensorflow 中，用 tf.nn.sigmoid()表示  

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_08.png" />

③ 激活函数 tanh：在 Tensorflow 中，用 tf.nn.tanh()表示  

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_09.png" />

√神经网络的复杂度：可用神经网络的层数和神经网络中待优化参数个数表示

 √神经网路的层数：一般不计入输入层，**层数 = n 个隐藏层 + 1 个输出层**  

 √神经网路待优化的参数：神经网络中所有参数 w 的个数 + 所有参数 b 的个数  

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_10.png" />

√损失函数（loss）：用来表示预测值（y）与已知答案（y_）的差距。在训练神经网络时，通过不断 改变神经网络中所有参数，使损失函数不断减小，从而训练出更高准确率的神经网络模型。 _

_√常用的损失函数有均方误差、自定义和交叉熵等。

 √均方误差 mse：n 个样本的预测值 y 与已知答案 y_之差的平方和，再求平均值。 

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_11.png" />

√**交叉熵(Cross Entropy)：表示两个概率分布之间的距离。交叉熵越大，两个概率分布距离越远，两 个概率分布越相异；交叉熵越小，两个概率分布距离越近，两个概率分布越相似。** 

交叉熵计算公式：𝐇(𝐲_ , 𝐲) = −∑𝐲_ ∗ 𝒍𝒐𝒈 𝒚 

用 Tensorflow 函数表示为 

**ce= -tf.reduce_mean(y_* tf.log(tf.clip_by_value(y, 1e-12, 1.0)))**  

例如：

 两个神经网络模型解决二分类问题中，已知标准答案为 y_ = (1, 0)，第一个神经网络模型预测结果为 

y1=(0.6, 0.4)，第二个神经网络模型预测结果为 y2=(0.8, 0.2)，判断哪个神经网络模型预测的结果更接 近标准答案。 

根据交叉熵的计算公式得：

 H1((1,0),(0.6,0.4)) = -(1*log0.6 + 0*log0.4) ≈ -(-0.222 + 0) = 0.222

 H2((1,0),(0.8,0.2)) = -(1*log0.8 + 0*log0.2) ≈ -(-0.097 + 0) = 0.097 

由于 0.222>0.097，所以预测结果 y2 与标准答案 y_更接近，y2 预测更准确 

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_12.png" />

**softmax 函数**应用：

在 n 分类中，模型会有 n 个输出，即 y1,y2…yn，其中 yi 表示第 i 种情况出现的可 能性大小。将 n 个输出经过 softmax 函数，可得到符合概率分布的分类结果。

 √在 Tensorflow 中，一般让模型的输出经过 sofemax 函数，以获得输出分类的概率分布，再与标准 答案对比，求出交叉熵，得到损失函数，用如下函数实现：

 **ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))** 

**cem = tf.reduce_mean(ce)** 

计算出的结果就是当前计算出的预测值和标准值之间的差距，也就是交叉熵

**√学习率** learning_rate：表示了每次参数更新的幅度大小。学习率过大，会导致待优化的参数在最 小值附近波动，不收敛；学习率过小，会导致待优化的参数收敛缓慢。 

在训练过程中，参数的更新向着损失函数梯度下降的方向。 参数的更新公式为：

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_13.png" />
<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_14.png" />

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_15.png" />

代码如下：

```python
# coding: utf-8
# 设损失函数 loss=(w+1)^2,令w初值是常数5。反向传播就是求最优w，即求最小loss对应的w值
import tensorflow as tf
# 定义待优化参数w初值赋5
w = tf.Variable(tf.constant(5, dtype=tf.float32))
# 定义损失函数loss
loss = tf.square(w+1)
# 定义反向传播方法
train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)
# 生成会话，训练40轮
with tf.Session() as sess:
    inin_op = tf.global_variables_initializer()
    sess.run(inin_op)
    for i in range(40):
        sess.run(train_step)
        w_val = sess.run(w)
        loss_val = sess.run(loss)
        print("After %s steps: w is %f, loss is %f." %(i, w_val, loss_val))
"""
F:\pycharmcache\venv\Scripts\python.exe F:/pycharmcache/tensorflow学习/15_学习率.py
WARNING:tensorflow:From F:\pycharmcache\venv\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-03-10 20:20:33.971784: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
After 0 steps: w is 2.600000, loss is 12.959999.
After 1 steps: w is 1.160000, loss is 4.665599.
After 2 steps: w is 0.296000, loss is 1.679616.
After 3 steps: w is -0.222400, loss is 0.604662.
After 4 steps: w is -0.533440, loss is 0.217678.
After 5 steps: w is -0.720064, loss is 0.078364.
After 6 steps: w is -0.832038, loss is 0.028211.
After 7 steps: w is -0.899223, loss is 0.010156.
After 8 steps: w is -0.939534, loss is 0.003656.
After 9 steps: w is -0.963720, loss is 0.001316.
After 10 steps: w is -0.978232, loss is 0.000474.
After 11 steps: w is -0.986939, loss is 0.000171.
After 12 steps: w is -0.992164, loss is 0.000061.
After 13 steps: w is -0.995298, loss is 0.000022.
After 14 steps: w is -0.997179, loss is 0.000008.
After 15 steps: w is -0.998307, loss is 0.000003.
After 16 steps: w is -0.998984, loss is 0.000001.
After 17 steps: w is -0.999391, loss is 0.000000.
After 18 steps: w is -0.999634, loss is 0.000000.
After 19 steps: w is -0.999781, loss is 0.000000.
After 20 steps: w is -0.999868, loss is 0.000000.
After 21 steps: w is -0.999921, loss is 0.000000.
After 22 steps: w is -0.999953, loss is 0.000000.
After 23 steps: w is -0.999972, loss is 0.000000.
After 24 steps: w is -0.999983, loss is 0.000000.
After 25 steps: w is -0.999990, loss is 0.000000.
After 26 steps: w is -0.999994, loss is 0.000000.
After 27 steps: w is -0.999996, loss is 0.000000.
After 28 steps: w is -0.999998, loss is 0.000000.
After 29 steps: w is -0.999999, loss is 0.000000.
After 30 steps: w is -0.999999, loss is 0.000000.
After 31 steps: w is -1.000000, loss is 0.000000.
After 32 steps: w is -1.000000, loss is 0.000000.
After 33 steps: w is -1.000000, loss is 0.000000.
After 34 steps: w is -1.000000, loss is 0.000000.
After 35 steps: w is -1.000000, loss is 0.000000.
After 36 steps: w is -1.000000, loss is 0.000000.
After 37 steps: w is -1.000000, loss is 0.000000.
After 38 steps: w is -1.000000, loss is 0.000000.
After 39 steps: w is -1.000000, loss is 0.000000.

Process finished with exit code 0

"""
```

由图可知，损失函数 loss 的最小值会在(-1,0)处得到，此时损失函数的导数为 0,得到最终参数 w = -1。 

**√学习率的设置** 

学习率过大，会导致待优化的参数在最小值附近波动，不收敛；学习率过小，会导致待优化的参数收 敛缓慢 

例如：

 ① 对于上例的损失函数 loss = (w + 1)2。

则将上述代码中学习率修改为 1，其余内容不变。 实验结果如下： 

```python 
train_step = tf.train.GradientDescentOptimizer(1).minimize(loss)
```



```python
"""
F:\pycharmcache\venv\Scripts\python.exe F:/pycharmcache/tensorflow学习/15_学习率.py
WARNING:tensorflow:From F:\pycharmcache\venv\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-03-10 20:24:36.066876: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
After 0 steps: w is -7.000000, loss is 36.000000.
After 1 steps: w is 5.000000, loss is 36.000000.
After 2 steps: w is -7.000000, loss is 36.000000.
After 3 steps: w is 5.000000, loss is 36.000000.
After 4 steps: w is -7.000000, loss is 36.000000.
After 5 steps: w is 5.000000, loss is 36.000000.
After 6 steps: w is -7.000000, loss is 36.000000.
After 7 steps: w is 5.000000, loss is 36.000000.
After 8 steps: w is -7.000000, loss is 36.000000.
After 9 steps: w is 5.000000, loss is 36.000000.
After 10 steps: w is -7.000000, loss is 36.000000.
After 11 steps: w is 5.000000, loss is 36.000000.
After 12 steps: w is -7.000000, loss is 36.000000.
After 13 steps: w is 5.000000, loss is 36.000000.
After 14 steps: w is -7.000000, loss is 36.000000.
After 15 steps: w is 5.000000, loss is 36.000000.
After 16 steps: w is -7.000000, loss is 36.000000.
After 17 steps: w is 5.000000, loss is 36.000000.
After 18 steps: w is -7.000000, loss is 36.000000.
After 19 steps: w is 5.000000, loss is 36.000000.
After 20 steps: w is -7.000000, loss is 36.000000.
After 21 steps: w is 5.000000, loss is 36.000000.
After 22 steps: w is -7.000000, loss is 36.000000.
After 23 steps: w is 5.000000, loss is 36.000000.
After 24 steps: w is -7.000000, loss is 36.000000.
After 25 steps: w is 5.000000, loss is 36.000000.
After 26 steps: w is -7.000000, loss is 36.000000.
After 27 steps: w is 5.000000, loss is 36.000000.
After 28 steps: w is -7.000000, loss is 36.000000.
After 29 steps: w is 5.000000, loss is 36.000000.
After 30 steps: w is -7.000000, loss is 36.000000.
After 31 steps: w is 5.000000, loss is 36.000000.
After 32 steps: w is -7.000000, loss is 36.000000.
After 33 steps: w is 5.000000, loss is 36.000000.
After 34 steps: w is -7.000000, loss is 36.000000.
After 35 steps: w is 5.000000, loss is 36.000000.
After 36 steps: w is -7.000000, loss is 36.000000.
After 37 steps: w is 5.000000, loss is 36.000000.
After 38 steps: w is -7.000000, loss is 36.000000.
After 39 steps: w is 5.000000, loss is 36.000000.

Process finished with exit code 0
"""

```

则将上述代码中学习率修改为 0.001，其余内容不变。 实验结果如下：

```python
train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)
```

```python
"""
F:\pycharmcache\venv\Scripts\python.exe F:/pycharmcache/tensorflow学习/15_学习率.py
WARNING:tensorflow:From F:\pycharmcache\venv\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-03-10 20:27:20.014024: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
After 0 steps: w is 4.988000, loss is 35.856144.
After 1 steps: w is 4.976024, loss is 35.712864.
After 2 steps: w is 4.964072, loss is 35.570156.
After 3 steps: w is 4.952144, loss is 35.428020.
After 4 steps: w is 4.940240, loss is 35.286449.
After 5 steps: w is 4.928360, loss is 35.145447.
After 6 steps: w is 4.916503, loss is 35.005009.
After 7 steps: w is 4.904670, loss is 34.865124.
After 8 steps: w is 4.892860, loss is 34.725803.
After 9 steps: w is 4.881075, loss is 34.587044.
After 10 steps: w is 4.869313, loss is 34.448833.
After 11 steps: w is 4.857574, loss is 34.311172.
After 12 steps: w is 4.845859, loss is 34.174068.
After 13 steps: w is 4.834167, loss is 34.037510.
After 14 steps: w is 4.822499, loss is 33.901497.
After 15 steps: w is 4.810854, loss is 33.766029.
After 16 steps: w is 4.799233, loss is 33.631104.
After 17 steps: w is 4.787634, loss is 33.496712.
After 18 steps: w is 4.776059, loss is 33.362858.
After 19 steps: w is 4.764507, loss is 33.229538.
After 20 steps: w is 4.752978, loss is 33.096756.
After 21 steps: w is 4.741472, loss is 32.964497.
After 22 steps: w is 4.729989, loss is 32.832775.
After 23 steps: w is 4.718529, loss is 32.701576.
After 24 steps: w is 4.707092, loss is 32.570904.
After 25 steps: w is 4.695678, loss is 32.440750.
After 26 steps: w is 4.684287, loss is 32.311119.
After 27 steps: w is 4.672918, loss is 32.182003.
After 28 steps: w is 4.661572, loss is 32.053402.
After 29 steps: w is 4.650249, loss is 31.925320.
After 30 steps: w is 4.638949, loss is 31.797745.
After 31 steps: w is 4.627671, loss is 31.670683.
After 32 steps: w is 4.616416, loss is 31.544128.
After 33 steps: w is 4.605183, loss is 31.418077.
After 34 steps: w is 4.593973, loss is 31.292530.
After 35 steps: w is 4.582785, loss is 31.167484.
After 36 steps: w is 4.571619, loss is 31.042938.
After 37 steps: w is 4.560476, loss is 30.918892.
After 38 steps: w is 4.549355, loss is 30.795341.
After 39 steps: w is 4.538256, loss is 30.672281.

Process finished with exit code 0

"""
```

由运行结果可知，损失函数 loss 值缓慢下降，w 值也在小幅度变化，收敛缓慢。 大了或者是小了都会有影响。

**√指数衰减学习率：学习率随着训练轮数变化而动态更新** 

**学习率计算公式如下：** 

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_16.png" />

其中，LEARNING_RATE_BASE 为学习率初始值

LEARNING_RATE_DECAY 为学习率衰减率

global_step 记 录了当前训练轮数，为不可训练型参数(0,trainable = False)

学习率 learning_rate 更新频率为输入数据集总样本数除以每 次喂入样本数

若 staircase 设置为 True 时，表示 global_step/learning rate step 取整数，学习率阶梯型衰减；

若 staircase 设置为 false 时，学习率会是一条平滑下降的曲线 

在本例中，模型训练过程不设定固定的学习率，使用指数衰减学习率进行训练。其中，学习率初值设 置为 0.1，学习率衰减率设置为 0.99，BATCH_SIZE 设置为 1。 

代码如下： 

```python 
# coding: utf-8
# coding: utf-8
# 设损失函数 loss=(w+1)^2,令w初值是常数10。反向传播就是求最优w，即求最小loss对应的w值
# 使用指数衰减的学习率，在迭代初期得到较高的下降速度，可以在较小的训练轮数下去的更有收敛度
import tensorflow as tf
LEARNING_RATE_BASE = 0.1 # 最初的学习率
LEARNING_RATE_DECAY = 0.99 # 学习率衰减率
LEARNING_RATE_STEP = 1 # 喂入多少轮BATCH_SIZE后，更新一次学习率，一般设置为：总样本数/BATCH_SIZE
# 运行了几轮BATCH_SIZE的计数器，初始给0，设为不被训练
global_step = tf.Variable(0, trainable=False)
# 定义指数下降学习率
learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, LEARNING_RATE_STEP,
                                           LEARNING_RATE_DECAY, staircase=True)
# 定义待优化参数，初值给10
w = tf.Variable(tf.constant(10, dtype=tf.float32))
# 定义损失函数loss
loss = tf.square(w+1)
# 定义反向传播方法
train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=
                                                                       global_step)
# 生成会话，训练40轮
with tf.Session() as sess:
    inin_op = tf.global_variables_initializer()
    sess.run(inin_op)
    for i in range(40):
        sess.run(train_step)
        w_val = sess.run(w)
        loss_val = sess.run(loss)
        print("After %s steps: w is %f, loss is %f." %(i, w_val, loss_val))
"""
F:\pycharmcache\venv\Scripts\python.exe F:/pycharmcache/tensorflow学习/16_加入指数衰减学习率.py
WARNING:tensorflow:From F:\pycharmcache\venv\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-03-10 21:01:38.994750: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
After 0 steps: w is 7.800000, loss is 77.440002.
After 1 steps: w is 6.057600, loss is 49.809719.
After 2 steps: w is 4.674169, loss is 32.196194.
After 3 steps: w is 3.573041, loss is 20.912704.
After 4 steps: w is 2.694472, loss is 13.649122.
After 5 steps: w is 1.991791, loss is 8.950810.
After 6 steps: w is 1.428448, loss is 5.897361.
After 7 steps: w is 0.975754, loss is 3.903603.
After 8 steps: w is 0.611130, loss is 2.595741.
After 9 steps: w is 0.316771, loss is 1.733887.
After 10 steps: w is 0.078598, loss is 1.163374.
After 11 steps: w is -0.114544, loss is 0.784033.
After 12 steps: w is -0.271515, loss is 0.530691.
After 13 steps: w is -0.399367, loss is 0.360760.
After 14 steps: w is -0.503727, loss is 0.246287.
After 15 steps: w is -0.589091, loss is 0.168846.
After 16 steps: w is -0.659066, loss is 0.116236.
After 17 steps: w is -0.716543, loss is 0.080348.
After 18 steps: w is -0.763853, loss is 0.055765.
After 19 steps: w is -0.802872, loss is 0.038859.
After 20 steps: w is -0.835119, loss is 0.027186.
After 21 steps: w is -0.861821, loss is 0.019094.
After 22 steps: w is -0.883974, loss is 0.013462.
After 23 steps: w is -0.902390, loss is 0.009528.
After 24 steps: w is -0.917728, loss is 0.006769.
After 25 steps: w is -0.930527, loss is 0.004827.
After 26 steps: w is -0.941226, loss is 0.003454.
After 27 steps: w is -0.950187, loss is 0.002481.
After 28 steps: w is -0.957706, loss is 0.001789.
After 29 steps: w is -0.964026, loss is 0.001294.
After 30 steps: w is -0.969348, loss is 0.000940.
After 31 steps: w is -0.973838, loss is 0.000684.
After 32 steps: w is -0.977631, loss is 0.000500.
After 33 steps: w is -0.980842, loss is 0.000367.
After 34 steps: w is -0.983565, loss is 0.000270.
After 35 steps: w is -0.985877, loss is 0.000199.
After 36 steps: w is -0.987844, loss is 0.000148.
After 37 steps: w is -0.989520, loss is 0.000110.
After 38 steps: w is -0.990951, loss is 0.000082.
After 39 steps: w is -0.992174, loss is 0.000061.

Process finished with exit code 0

"""
```

结果可以看出，随着训练轮数增加学习率在不断减小。 

**√滑动平均：记录了一段时间内模型中所有参数 w 和 b 各自的平均值。利用滑动平均值可以增强模 型的泛化能力。** 
针对所有参数：w和b

（就像是给参数加了一个影子，参数变化，影子缓慢追随）

√滑动平均值（影子）计算公式： 影子 = 衰减率 * 影子 +（1 - 衰减率）* 参数  

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_17.png" />

√用 Tesnsorflow 函数表示为：

 **√ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY，global_step)** 

其中，MOVING_AVERAGE_DECAY 表示滑动平均衰减率，一般会赋接近 1 的值，global_step 表示当前 训练了多少轮。

 **√ema_op = ema.apply(tf.trainable_variables())**

 其中，ema.apply()函数实现对括号内参数求滑动平均，tf.trainable_variables()函数实现把所有 待训练参数汇总为列表。

 **√with tf.control_dependencies([train_step, ema_op]): train_op =                                                         	               ************tf.no_op(name='train')** 

其中，该函数实现将滑动平均和训练过程同步运行。

 查看模型中参数的平均值，可以用 ema.average()函数。 

例如：

 在神经网络模型中，将 MOVING_AVERAGE_DECAY 设置为 0.99，参数 w1 设置为 0，w1 的滑动平均值设 置为 0。

 ①开始时，轮数 global_step 设置为 0，参数 w1 更新为 1，则 w1 的滑动平均值为： w1 滑动平均值=min(0.99,1/10)*0+(1– min(0.99,1/10)*1 = 0.9

 2. 当轮数 global_step 设置为 100 时，参数 w1 更新为 10，以下代码 global_step 保持为 100，每 次执行滑动平均操作影子值更新，则滑动平均值变为： 

w1 滑动平均值=min(0.99,101/110)*0.9+(1– min(0.99,101/110)*10 = 0.826+0.818=1.644

 ③再次运行，参数 w1 更新为 1.644，则滑动平均值变为：

 w1 滑动平均值=min(0.99,101/110)*1.644+(1– min(0.99,101/110)*10 = 2.328 

④再次运行，参数 w1 更新为 2.328，则滑动平均值： w1 滑动平均值=2.956

 代码如下： 

```python
# coding: utf-8
import tensorflow as tf

# 1. 定义变量及滑动平均类
# 定义一个32位浮点变量，初始值位0.0，这个代码就是不断更新w1参数，优化w1参数，滑动平均做了>个w1的影子
w1 = tf.Variable(0, dtype=tf.float32)
# 定义num_updates(NN的迭代轮数),初始值为0，不可被优化（训练），这个参数不训练
global_step = tf.Variable(0, trainable=False)
# 实例化滑动平均类，给删减率为0.99，当前轮数global_step
MOVING_AVERAGE_DECAY = 0.99
ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)
# ema.apply 后的括号里是更新列表，每次运行sess.run(ema_op)时，对更新列表中的元素求滑动平均值
# 在实际应用中会使用tf.trainable_variables()自动将所有待训练的参数汇总为列表
# ema_op = ema.apply([w1])
ema_op = ema.apply(tf.trainable_variables())

# 2.查看不同迭代中变量取值的变化
with tf.Session() as sess:
    # 初始化
    init_op = tf.global_variables_initializer()
    sess.run(init_op)
    # 用ema.average(w1)获取w1滑动平均值 (要运行多个节点，作为列表中的元素列出，写在sess.run中）
    # 打印出当前参数w1和w1滑动平均值
    print(sess.run([w1, ema.average(w1)]))
    # 参数w1的值赋为1
    sess.run(tf.assign(w1, 1))
    sess.run(ema_op)
    print(sess.run([w1, ema.average(w1)]))

    # 更新step和w1的值，模拟出100轮迭代后，参数w1变为10
    sess.run(tf.assign(global_step, 100))
    sess.run(tf.assign(w1, 10))
    sess.run(ema_op)
    print(sess.run([w1, ema.average(w1)]))

    # 每次sess.run会更新一次w1的滑动平均值
    sess.run(ema_op)
    print(sess.run([w1, ema.average(w1)]))

    sess.run(ema_op)
    print(sess.run([w1, ema.average(w1)]))

    sess.run(ema_op)
    print(sess.run([w1, ema.average(w1)]))

    sess.run(ema_op)
    print(sess.run([w1, ema.average(w1)]))

    sess.run(ema_op)
    print(sess.run([w1, ema.average(w1)]))

"""
F:\pycharmcache\venv\Scripts\python.exe F:/pycharmcache/tensorflow学习/17_加入滑动平均类.py
WARNING:tensorflow:From F:\pycharmcache\venv\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-03-11 10:32:47.740841: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
[0.0, 0.0]
[1.0, 0.9]
[10.0, 1.6445453]
[10.0, 2.3281732]
[10.0, 2.955868]
[10.0, 3.532206]
[10.0, 4.061389]
[10.0, 4.547275]

Process finished with exit code 0

"""
```

从运行结果可知，最初参数 w1 和滑动平均值都是 0；参数 w1 设定为 1 后，滑动平均值变为 0.9； 当迭代轮数更新为 100 轮时，参数 w1 更新为 10 后，滑动平均值变为 1.644。随后每执行一次，参数 w1 的滑动平均值都向参数 w1 靠近。可见，滑动平均追随参数的变化而变化。 

√过拟合：

神经网络模型在训练数据集上的准确率较高，在新的数据进行预测或分类时准确率较 低，说明模型的泛化能力差。

 √**正则化**：在损失函数中给每个参数 w 加上权重，引入模型复杂度指标，从而抑制模型噪声，减小 过拟合。(一般不正则化b)

 使用正则化后，损失函数 loss 变为两项之和：

 loss = loss(y 与 y_) + REGULARIZER*loss(w) 

其中，第一项是预测结果与标准答案之间的差距，如之前讲过的交叉熵、均方误差等；第二项是正则 化计算结果。 

√正则化计算方法：

 ① L1 正则化： 𝒍𝒐𝒔𝒔𝑳𝟏 = ∑𝒊 |𝒘𝒊 |

 用 Tesnsorflow 函数表示:loss(w) = tf.contrib.layers.l1_regularizer(REGULARIZER)(w) ② L2 正则化： 𝒍𝒐𝒔𝒔𝑳𝟐 = ∑𝒊 |𝒘𝒊 | 𝟐

 用 Tesnsorflow 函数表示:loss(w) = tf.contrib.layers.l2_regularizer(REGULARIZER)(w) √用 Tesnsorflow 函数实现正则化：

 tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w)

 loss = cem + tf.add_n(tf.get_collection('losses')) 

例如：

 用 300 个符合正态分布的点 X[x0, x1]作为数据集，根据点 X[x0, x1]计算生成标注 Y_，将数据集 标注为红色点和蓝色点。_

_ 标注规则为：当 x0 2 + x1 2 < 2 时，y_=1，标注为红色；当 x0 2 + x1 2 ≥2 时，y_=0，标注为蓝色。 我们分别用无正则化和有正则化两种方法，拟合曲线，把红色点和蓝色点分开。在实际分类时， 如果前向传播输出的预测值 y 接近 1 则为红色点概率越大，接近 0 则为蓝色点概率越大，输出的预 测值 y 为 0.5 是红蓝点概率分界线。

 在本例子中，我们使用了之前未用过的模块与函数：

 √matplotlib 模块：Python 中的可视化工具模块，实现函数可视化 

终端安装指令：sudo pip install matplotlib

 √函数 plt.scatter（）：利用指定颜色实现点(x,y)的可视化 

plt.scatter (x 坐标, y 坐标, c=”颜色”) 

plt.show()  

 √收集规定区域内所有的网格坐标点：

 xx, yy = np.mgrid[起:止:步长, 起:止:步长] #找到规定区域以步长为分辨率的行列网格坐标点

 grid = np.c_[xx.ravel(), yy.ravel()] #收集规定区域内所有的网格坐标点

 √plt.contour()函数：告知 x、y 坐标和各点高度，用 levels 指定高度的点描上颜色 plt.contour (x 轴坐标值, y 轴坐标值, 该点的高度, levels=[等高线的高度]) 

plt.show()

 本例代码如下：

```python
# coding: utf-8
# 0导入模块，生成模拟数据集
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
BATCH_SIZE= 30
seed = 2
# 基于seed产生随机数
rdm = np.random.RandomState(seed)
# 随机数返回300行2列的矩阵，表示300组坐标点（x0,x1)作为输入数据集
X = rdm.randn(300,2)
# 从X这个300行2列的矩阵中取出一行，判断如果两个坐标的平方和小于2，给Y赋值1，其余赋值0
# 作为输入数据集的标签（正确答案）
Y_ = [int(x0*x0 + x1*x1 < 2) for (x0, x1) in X]
# 遍历Y中的每个元素，1赋值'red'其余赋值'blue',这样可视化显示时人可以直观区分
Y_c = [['red' if y else 'blue'] for y in Y_]
# 对数据集X和标签Y进行shape整理，第一个元素为-1表示，随第二个参数计算得到，第二个元素表示多少列，把X整理为n
# 行2列，把Y整理为n行1列
X = np.vstack(X).reshape(-1,2)
Y_ = np.vstack(Y_).reshape(-1,1)
print(X)
print(Y_)
print(Y_c)
# 用plt.scatter画出数据集X各行中第0列元素和第1列元素的点，即各行的（x0,x1)，用各行Y_c对应的值表示颜色（
# c是color的缩写）
plt.scatter(X[:, 0], X[:, 1], c=np.squeeze(Y_c))
plt.show()

# 定义神经网络的输入、参数和输出，定义前向传播过程


def get_weight(shape, regularizer):
    w = tf.Variable(tf.random_normal(shape), dtype=tf.float32)
    tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w))
    return w


def get_bias(shape):
    b = tf.Variable(tf.constant(0.01,shape=shape))
    return b


x = tf.placeholder(tf.float32, shape=(None, 2))
y_ = tf.placeholder(tf.float32, shape=(None, 1))
w1 = get_weight([2,11], 0.01)
b1 = get_bias([11])
y1 = tf.nn.relu(tf.matmul(x, w1) + b1)

w2 = get_weight([11,1],0.01)
b2 = get_bias([1])
y = tf.matmul(y1, w2) + b2 # 输出层不过激活

# 定义损失函数
loos_mse = tf.reduce_mean(tf.square(y - y_))
loss_total = loos_mse + tf.add_n(tf.get_collection('losses'))
# 定义反向传播算法：不含正则化
train_step = tf.train.AdamOptimizer(0.0001).minimize(loos_mse)

with tf.Session() as sess:
    init_op = tf.global_variables_initializer()
    sess.run(init_op)
    STEPS = 40000
    for i in range(STEPS):
        start = (i * BATCH_SIZE) % 300
        end = start + BATCH_SIZE
        sess.run(train_step, feed_dict={x:X[start:end], y_:Y_[start:end]})
        if i % 2000 == 0:
            loos_mse_v = sess.run(loos_mse, feed_dict={x:X, y_:Y_})
            print("After %d steps, loss is: %f"%(i,loos_mse_v))
    # xx在-3到3之间以步长为0.01，yy在-3到3之间以步长0.01，生成二维网络坐标点
    xx, yy = np.mgrid[-3:3:.01, -3:3:.01]
    grid = np.c_[xx.ravel(), yy.ravel()]
    probs = sess.run(y, feed_dict={x:grid})
    probs = probs.reshape(xx.shape)
    print("w1:\n",sess.run(w1))
    print("b1:\n",sess.run(b1))
    print("w2:\n",sess.run(w2))
    print("b2:\n",sess.run(b2))
plt.scatter(X[:, 0], X[:, 1], c=np.squeeze(Y_c))
plt.contour(xx, yy, probs, levels=[.5])
plt.show()

"""
F:\pycharmcache\venv\Scripts\python.exe F:/pycharmcache/tensorflow学习/18_加入正则化.py
[[-4.16757847e-01 -5.62668272e-02]
 [-2.13619610e+00  1.64027081e+00]
 [-1.79343559e+00 -8.41747366e-01]
 [ 5.02881417e-01 -1.24528809e+00]
 [-1.05795222e+00 -9.09007615e-01]
 [ 5.51454045e-01  2.29220801e+00]
 [ 4.15393930e-02 -1.11792545e+00]
 [ 5.39058321e-01 -5.96159700e-01]
 [-1.91304965e-02  1.17500122e+00]
 [-7.47870949e-01  9.02525097e-03]
 [-8.78107893e-01 -1.56434170e-01]
 [ 2.56570452e-01 -9.88779049e-01]
 [-3.38821966e-01 -2.36184031e-01]
 [-6.37655012e-01 -1.18761229e+00]
 [-1.42121723e+00 -1.53495196e-01]
 [-2.69056960e-01  2.23136679e+00]
 [-2.43476758e+00  1.12726505e-01]
 [ 3.70444537e-01  1.35963386e+00]
 [ 5.01857207e-01 -8.44213704e-01]
 [ 9.76147160e-06  5.42352572e-01]
 [-3.13508197e-01  7.71011738e-01]
 [-1.86809065e+00  1.73118467e+00]
 [ 1.46767801e+00 -3.35677339e-01]
 [ 6.11340780e-01  4.79705919e-02]
 [-8.29135289e-01  8.77102184e-02]
 [ 1.00036589e+00 -3.81092518e-01]
 [-3.75669423e-01 -7.44707629e-02]
 [ 4.33496330e-01  1.27837923e+00]
 [-6.34679305e-01  5.08396243e-01]
 [ 2.16116006e-01 -1.85861239e+00]
 [-4.19316482e-01 -1.32328898e-01]
 [-3.95702397e-02  3.26003433e-01]
 [-2.04032305e+00  4.62555231e-02]
 [-6.77675577e-01 -1.43943903e+00]
 [ 5.24296430e-01  7.35279576e-01]
 [-6.53250268e-01  8.42456282e-01]
 [-3.81516482e-01  6.64890091e-02]
 [-1.09873895e+00  1.58448706e+00]
 [-2.65944946e+00 -9.14526229e-02]
 [ 6.95119605e-01 -2.03346655e+00]
 [-1.89469265e-01 -7.72186654e-02]
 [ 8.24703005e-01  1.24821292e+00]
 [-4.03892269e-01 -1.38451867e+00]
 [ 1.36723542e+00  1.21788563e+00]
 [-4.62005348e-01  3.50888494e-01]
 [ 3.81866234e-01  5.66275441e-01]
 [ 2.04207979e-01  1.40669624e+00]
 [-1.73795950e+00  1.04082395e+00]
 [ 3.80471970e-01 -2.17135269e-01]
 [ 1.17353150e+00 -2.34360319e+00]
 [ 1.16152149e+00  3.86078048e-01]
 [-1.13313327e+00  4.33092555e-01]
 [-3.04086439e-01  2.58529487e+00]
 [ 1.83533272e+00  4.40689872e-01]
 [-7.19253841e-01 -5.83414595e-01]
 [-3.25049628e-01 -5.60234506e-01]
 [-9.02246068e-01 -5.90972275e-01]
 [-2.76179492e-01 -5.16883894e-01]
 [-6.98589950e-01 -9.28891925e-01]
 [ 2.55043824e+00 -1.47317325e+00]
 [-1.02141473e+00  4.32395701e-01]
 [-3.23580070e-01  4.23824708e-01]
 [ 7.99179995e-01  1.26261366e+00]
 [ 7.51964849e-01 -9.93760983e-01]
 [ 1.10914328e+00 -1.76491773e+00]
 [-1.14421297e-01 -4.98174194e-01]
 [-1.06079904e+00  5.91666521e-01]
 [-1.83256574e-01  1.01985473e+00]
 [-1.48246548e+00  8.46311892e-01]
 [ 4.97940148e-01  1.26504175e-01]
 [-1.41881055e+00 -2.51774118e-01]
 [-1.54667461e+00 -2.08265194e+00]
 [ 3.27974540e+00  9.70861320e-01]
 [ 1.79259285e+00 -4.29013319e-01]
 [ 6.96197980e-01  6.97416272e-01]
 [ 6.01515814e-01  3.65949071e-03]
 [-2.28247558e-01 -2.06961226e+00]
 [ 6.10144086e-01  4.23496900e-01]
 [ 1.11788673e+00 -2.74242089e-01]
 [ 1.74181219e+00 -4.47500876e-01]
 [-1.25542722e+00  9.38163671e-01]
 [-4.68346260e-01 -1.25472031e+00]
 [ 1.24823646e-01  7.56502143e-01]
 [ 2.41439629e-01  4.97425649e-01]
 [ 4.10869262e+00  8.21120877e-01]
 [ 1.53176032e+00 -1.98584577e+00]
 [ 3.65053516e-01  7.74082033e-01]
 [-3.64479092e-01 -8.75979478e-01]
 [ 3.96520159e-01 -3.14617436e-01]
 [-5.93755583e-01  1.14950057e+00]
 [ 1.33556617e+00  3.02629336e-01]
 [-4.54227855e-01  5.14370717e-01]
 [ 8.29458431e-01  6.30621967e-01]
 [-1.45336435e+00 -3.38017777e-01]
 [ 3.59133332e-01  6.22220414e-01]
 [ 9.60781945e-01  7.58370347e-01]
 [-1.13431848e+00 -7.07420888e-01]
 [-1.22142917e+00  1.80447664e+00]
 [ 1.80409807e-01  5.53164274e-01]
 [ 1.03302907e+00 -3.29002435e-01]
 [-1.15100294e+00 -4.26522471e-01]
 [-1.48147191e-01  1.50143692e+00]
 [ 8.69598198e-01 -1.08709057e+00]
 [ 6.64221413e-01  7.34884668e-01]
 [-1.06136574e+00 -1.08516824e-01]
 [-1.85040397e+00  3.30488064e-01]
 [-3.15693210e-01 -1.35000210e+00]
 [-6.98170998e-01  2.39951198e-01]
 [-5.52949440e-01  2.99526813e-01]
 [ 5.52663696e-01 -8.40443012e-01]
 [-3.12270670e-01  2.14467809e+00]
 [ 1.21105582e-01 -8.46828752e-01]
 [ 6.04624490e-02 -1.33858888e+00]
 [ 1.13274608e+00  3.70304843e-01]
 [ 1.08580640e+00  9.02179395e-01]
 [ 3.90296450e-01  9.75509412e-01]
 [ 1.91573647e-01 -6.62209012e-01]
 [-1.02351498e+00 -4.48174823e-01]
 [-2.50545813e+00  1.82599446e+00]
 [-1.71406741e+00 -7.66395640e-02]
 [-1.31756727e+00 -2.02559359e+00]
 [-8.22453750e-02 -3.04666585e-01]
 [-1.59724130e-01  5.48946560e-01]
 [-6.18375485e-01  3.78794466e-01]
 [ 5.13251444e-01 -3.34844125e-01]
 [-2.83519516e-01  5.38424263e-01]
 [ 5.72509465e-02  1.59088487e-01]
 [-2.37440268e+00  5.85199353e-02]
 [ 3.76545911e-01 -1.35479764e-01]
 [ 3.35908395e-01  1.90437591e+00]
 [ 8.53644334e-02  6.65334278e-01]
 [-8.49995503e-01 -8.52341797e-01]
 [-4.79985112e-01 -1.01964910e+00]
 [-7.60113841e-03 -9.33830661e-01]
 [-1.74996844e-01 -1.43714343e+00]
 [-1.65220029e+00 -6.75661789e-01]
 [-1.06706712e+00 -6.52931145e-01]
 [-6.12094750e-01 -3.51262461e-01]
 [ 1.04547799e+00  1.36901602e+00]
 [ 7.25353259e-01 -3.59474459e-01]
 [ 1.49695179e+00 -1.53111111e+00]
 [-2.02336394e+00  2.67972576e-01]
 [-2.20644541e-03 -1.39291883e-01]
 [ 3.25654693e-02 -1.64056022e+00]
 [-1.15669917e+00  1.23403468e+00]
 [ 1.02818490e+00 -7.21879726e-01]
 [ 1.93315697e+00 -1.07079633e+00]
 [-5.71381608e-01  2.92432067e-01]
 [-1.19499989e+00 -4.87930544e-01]
 [-1.73071165e-01 -3.95346401e-01]
 [ 8.70840765e-01  5.92806797e-01]
 [-1.09929731e+00 -6.81530644e-01]
 [ 1.80066685e-01 -6.69310440e-02]
 [-7.87749540e-01  4.24753672e-01]
 [ 8.19885117e-01 -6.31118683e-01]
 [ 7.89059649e-01 -1.62167380e+00]
 [-1.61049926e+00  4.99939764e-01]
 [-8.34515207e-01 -9.96959687e-01]
 [-2.63388077e-01 -6.77360492e-01]
 [ 3.27067038e-01 -1.45535944e+00]
 [-3.71519124e-01  3.16096597e+00]
 [ 1.09951013e-01 -1.91352322e+00]
 [ 5.99820429e-01  5.49384465e-01]
 [ 1.38378103e+00  1.48349243e-01]
 [-6.53541444e-01  1.40883398e+00]
 [ 7.12061227e-01 -1.80071604e+00]
 [ 7.47598942e-01 -2.32897001e-01]
 [ 1.11064528e+00 -3.73338813e-01]
 [ 7.86146070e-01  1.94168696e-01]
 [ 5.86204098e-01 -2.03872918e-02]
 [-4.14408598e-01  6.73134124e-02]
 [ 6.31798924e-01  4.17592731e-01]
 [ 1.61517627e+00  4.25606211e-01]
 [ 6.35363758e-01  2.10222927e+00]
 [ 6.61264168e-02  5.35558351e-01]
 [-6.03140792e-01  4.19576292e-02]
 [ 1.64191464e+00  3.11697707e-01]
 [ 1.45116990e+00 -1.06492788e+00]
 [-1.40084545e+00  3.07525527e-01]
 [-1.36963867e+00  2.67033724e+00]
 [ 1.24845030e+00 -1.24572655e+00]
 [-1.67168774e-01 -5.76610930e-01]
 [ 4.16021749e-01 -5.78472626e-02]
 [ 9.31887358e-01  1.46833213e+00]
 [-2.21320943e-01 -1.17315562e+00]
 [ 5.62669078e-01 -1.64515057e-01]
 [ 1.14485538e+00 -1.52117687e-01]
 [ 8.29789046e-01  3.36065952e-01]
 [-1.89044051e-01 -4.49328601e-01]
 [ 7.13524448e-01  2.52973487e+00]
 [ 8.37615794e-01 -1.31682403e-01]
 [ 7.07592866e-01  1.14053878e-01]
 [-1.28089518e+00  3.09846277e-01]
 [ 1.54829069e+00 -3.15828043e-01]
 [-1.12590378e+00  4.88496666e-01]
 [ 1.83094666e+00  9.40175993e-01]
 [ 1.01871705e+00  2.30237829e+00]
 [ 1.62109298e+00  7.12683273e-01]
 [-2.08703629e-01  1.37617991e-01]
 [-1.03352168e-01  8.48350567e-01]
 [-8.83125561e-01  1.54538683e+00]
 [ 1.45840073e-01 -4.00106056e-01]
 [ 8.15206041e-01 -2.07492237e+00]
 [-8.34437391e-01 -6.57718447e-01]
 [ 8.20564332e-01 -4.89157001e-01]
 [ 1.42496703e+00 -4.46857897e-01]
 [ 5.21109431e-01 -7.08194380e-01]
 [ 1.15553059e+00 -2.54530459e-01]
 [ 5.18924924e-01 -4.92994911e-01]
 [-1.08654815e+00 -2.30917497e-01]
 [ 1.09801004e+00 -1.01787805e+00]
 [-1.52939136e+00 -3.07987737e-01]
 [ 7.80754356e-01 -1.05583964e+00]
 [-5.43883381e-01  1.84301739e-01]
 [-3.30675843e-01  2.87208202e-01]
 [ 1.18952814e+00  2.12015479e-02]
 [-6.54096803e-02  7.66115904e-01]
 [-6.16350846e-02 -9.52897152e-01]
 [-1.01446306e+00 -1.11526396e+00]
 [ 1.91260068e+00 -4.52632031e-02]
 [ 5.76909718e-01  7.17805695e-01]
 [-9.38998998e-01  6.28775807e-01]
 [-5.64493432e-01 -2.08780746e+00]
 [-2.15050132e-01 -1.07502856e+00]
 [-3.37972149e-01  3.43212732e-01]
 [ 2.28253964e+00 -4.95778848e-01]
 [-1.63962832e-01  3.71622161e-01]
 [ 1.86521520e-01 -1.58429224e-01]
 [-1.08292956e+00 -9.56625520e-01]
 [-1.83376735e-01 -1.15980690e+00]
 [-6.57768362e-01 -1.25144841e+00]
 [ 1.12448286e+00 -1.49783981e+00]
 [ 1.90201722e+00 -5.80383038e-01]
 [-1.05491567e+00 -1.18275720e+00]
 [ 7.79480054e-01  1.02659795e+00]
 [-8.48666001e-01  3.31539648e-01]
 [-1.49591353e-01 -2.42440600e-01]
 [ 1.51197175e-01  7.65069481e-01]
 [-1.91663052e+00 -2.22734129e+00]
 [ 2.06689897e-01 -7.08763560e-02]
 [ 6.84759969e-01 -1.70753905e+00]
 [-9.86569665e-01  1.54353634e+00]
 [-1.31027053e+00  3.63433972e-01]
 [-7.94872445e-01 -4.05286267e-01]
 [-1.37775793e+00  1.18604868e+00]
 [-1.90382114e+00 -1.19814038e+00]
 [-9.10065643e-01  1.17645419e+00]
 [ 2.99210670e-01  6.79267178e-01]
 [-1.76606800e-02  2.36040923e-01]
 [ 4.94035871e-01  1.54627765e+00]
 [ 2.46857508e-01 -1.46877580e+00]
 [ 1.14709994e+00  9.55569845e-02]
 [-1.10743873e+00 -1.76286141e-01]
 [-9.82755667e-01  2.08668273e+00]
 [-3.44623671e-01 -2.00207923e+00]
 [ 3.03234433e-01 -8.29874845e-01]
 [ 1.28876941e+00  1.34925462e-01]
 [-1.77860064e+00 -5.00791490e-01]
 [-1.08816157e+00 -7.57855553e-01]
 [-6.43744900e-01 -2.00878453e+00]
 [ 1.96262894e-01 -8.75896370e-01]
 [-8.93609209e-01  7.51902355e-01]
 [ 1.89693224e+00 -6.29079151e-01]
 [ 1.81208553e+00 -2.05626574e+00]
 [ 5.62704887e-01 -5.82070757e-01]
 [-7.40029749e-02 -9.86496364e-01]
 [-5.94722499e-01 -3.14811843e-01]
 [-3.46940532e-01  4.11443516e-01]
 [ 2.32639090e+00 -6.34053128e-01]
 [-1.54409962e-01 -1.74928880e+00]
 [-2.51957930e+00  1.39116243e+00]
 [-1.32934644e+00 -7.45596414e-01]
 [ 2.12608498e-02  9.10917515e-01]
 [ 3.15276082e-01  1.86620821e+00]
 [-1.82497623e-01 -1.82826634e+00]
 [ 1.38955717e-01  1.19450165e-01]
 [-8.18899200e-01 -3.32639265e-01]
 [-5.86387955e-01  1.73451634e+00]
 [-6.12751558e-01 -1.39344202e+00]
 [ 2.79433757e-01 -1.82223127e+00]
 [ 4.27017458e-01  4.06987749e-01]
 [-8.44308241e-01 -5.59820113e-01]
 [-6.00520405e-01  1.61487324e+00]
 [ 3.94953220e-01 -1.20381347e+00]
 [-1.24747243e+00 -7.75462496e-02]
 [-1.33397514e-02 -7.68323250e-01]
 [ 2.91234010e-01 -1.97330948e-01]
 [ 1.07682965e+00  4.37410232e-01]
 [-9.31978663e-02  1.35631416e-01]
 [-8.82708822e-01  8.84744194e-01]
 [ 3.83204463e-01 -4.16994149e-01]
 [ 1.17796550e-01 -5.36685309e-01]
 [ 2.48718458e+00 -4.51361054e-01]
 [ 5.18836127e-01  3.64448005e-01]
 [-7.98348729e-01  5.65779713e-03]
 [-3.20934708e-01  2.49513550e-01]
 [ 2.56308392e-01  7.67625083e-01]
 [ 7.83020087e-01 -4.07063047e-01]
 [-5.24891667e-01 -5.89808683e-01]
 [-8.62531086e-01 -1.74287290e+00]]
[[1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [0]
 [1]
 [0]
 [1]
 [1]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]]
[['red'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['blue'], ['red'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['blue'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['blue'], ['blue'], ['red'], ['blue'], ['blue'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['blue'], ['blue'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue']]
WARNING:tensorflow:From F:\pycharmcache\venv\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.

WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

2019-03-11 18:53:46.688476: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
After 0 steps, loss is: 9.679747
After 2000 steps, loss is: 1.630938
After 4000 steps, loss is: 0.303394
After 6000 steps, loss is: 0.150194
After 8000 steps, loss is: 0.123724
After 10000 steps, loss is: 0.109868
After 12000 steps, loss is: 0.100317
After 14000 steps, loss is: 0.094361
After 16000 steps, loss is: 0.090736
After 18000 steps, loss is: 0.088312
After 20000 steps, loss is: 0.086347
After 22000 steps, loss is: 0.084526
After 24000 steps, loss is: 0.083241
After 26000 steps, loss is: 0.082133
After 28000 steps, loss is: 0.081313
After 30000 steps, loss is: 0.080637
After 32000 steps, loss is: 0.080137
After 34000 steps, loss is: 0.079734
After 36000 steps, loss is: 0.079305
After 38000 steps, loss is: 0.078953
w1:
 [[ 0.96199024 -0.5784183  -0.51273715 -0.685939   -1.236652   -1.8548323
  -0.8054253   0.7345482  -0.72819513 -1.4073868  -0.3324695 ]
 [ 0.66455346  0.03049601 -1.043516    0.685543   -0.17345296  0.70875156
  -1.5336182   0.16442642  1.0680308   0.54613787 -0.835796  ]]
b1:
 [ 0.09366781 -0.09222969  0.32802325  0.22673199  0.509652    0.06866115
 -0.01447262 -0.34689793 -0.0726015   0.07575417  0.07043334]
w2:
 [[ 0.48809904]
 [-1.4913772 ]
 [ 0.21193159]
 [-0.6359633 ]
 [ 0.69919974]
 [-1.0305494 ]
 [-1.2364928 ]
 [-1.416965  ]
 [-0.29577518]
 [ 1.7283639 ]
 [ 1.1308775 ]]
b2:
 [0.8180861]

Process finished with exit code 0

"""
```

 执行代码，效果如下： 首先，数据集实现可视化，x0 2 + x1 2 < 2 的点显示红色， x0 2 + x1 2 ≥2 的点显示蓝色，如图所示： 

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_18.png" />

接着，执行无正则化的训练过程，把红色的点和蓝色的点分开，生成曲线如下图所示： 

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_19.png" />

最后，执行有正则化的训练过程，把红色的点和蓝色的点分开，生成曲线如下图所示： 

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_20.png" />

对比无正则化与有正则化模型的训练结果，可看出有正则化模型的拟合曲线平滑，模型具有更好的泛 化能力。 

 **搭建模块化神经网络八股**

 √**前向传播：由输入到输出，搭建完整的网络结构** 

描述前向传播的过程需要定义三个函数：

 

```python
√def forward(x, regularizer): 

    w= 

     b= 

    y= 

    return y 
```

第一个函数 forward()完成网络结构的设计，从输入到输出搭建完整的网络结构，实现前向传播过程。 该函数中，参数 x 为输入，regularizer 为正则化权重，返回值为预测或分类结果 y。 

```
√def get_weight(shape, regularizer): 

	w = tf.Variable( )

    tf.add_to_collection('losses', 		                 (空格)tf.contrib.layers.l2_regularizer(regularizer)(w))

 return w 
```

第二个函数 get_weight()对参数 w 设定。该函数中，参数 shape 表示参数 w 的形状，regularizer 表示正则化权重，返回值为参数 w。其中，tf.variable()给 w 赋初值，tf.add_to_collection()表 示将参数 w 正则化损失加到总损失 losses 中。

 

```python
√def get_bias(shape): 

b = tf.Variable( ) 

return b
```

 

第三个函数 get_bias()对参数 b 进行设定。该函数中，参数 shape 表示参数 b 的形状,返回值为参数 b。其中，tf.variable()表示给 b 赋初值。

 √反向传播：训练网络，优化网络参数，提高模型准确性。 

```python
√def backward( ):

 x = tf.placeholder( )

 y_ = tf.placeholder( ) 

y = forward.forward(x, REGULARIZER) 

global_step = tf.Variable(0, trainable=False) 

loss =
```

 函数 backward()中，placeholder()实现对数据集 x 和标准答案 y_占位，forward.forward()实现前向 传播的网络结构，参数 global_step 表示训练轮数，设置为不可训练型参数。

 在训练网络模型时，常将正则化、指数衰减学习率和滑动平均这三个方法作为模型优化方法。

 √在 Tensorflow 中，正则化表示为：

 首先，计算预测结果与标准答案的损失值

 ①MSE： y 与 y_的差距(loss_mse) = tf.reduce_mean(tf.square(y-y_)) 

②交叉熵：ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))_

 y 与 y__的差距(cem) = tf.reduce_mean(ce)_

_③自定义：y 与 y__的差距 

其次，总损失值为预测结果与标准答案的损失值加上正则化项

 loss = y 与 y_的差距 + tf.add_n(tf.get_collection('losses')) _

_√在 Tensorflow 中，指数衰减学习率表示为：

 learning_rate = tf.train.exponential_decay(

 LEARNING_RATE_BASE,

 global_step, 

数据集总样本数 / BATCH_SIZE,

 LEARNING_RATE_DECAY, 

staircase=True) train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)

 √在 Tensorflow 中，滑动平均表示为：

 ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step) ema_op = ema.apply(tf.trainable_variables())

 with tf.control_dependencies([train_step, ema_op]): 

train_op = tf.no_op(name='train') 

其中，滑动平均和指数衰减学习率中的 global_step 为同一个参数。 

√用 with 结构初始化所有参数 

```python
with tf.Session() as sess:

    init_op = tf.global_variables_initializer()

    sess.run(init_op)

 for i in range(STEPS):

       sess.run(train_step, feed_dict={x: , y_: })_

               if i % 轮数 == 0:  

               print 
```

其中，with 结构用于初始化所有参数信息以及实现调用训练过程，并打印出 loss 值。 √判断 python 运行文件是否为主文件

```python
 if __name__=='__main__': 

backward()
```

 该部分用来判断 python 运行的文件是否为主文件。若是主文件，则执行 backword()函数。

 例如： 用 300 个符合正态分布的点 X[x0, x1]作为数据集，根据点 X[x0, x1]的不同进行标注 Y_，将数据集标 注为红色和蓝色。标注规则为：当 x0 2 + x1 2 < 2 时，y_=1，点 X 标注为红色；当 x0 2 + x1 2 ≥2 时， y_=0，点 X 标注为蓝色。我们加入指数衰减学习率优化效率，加入正则化提高泛化性，并使用模块化 设计方法，把红色点和蓝色点分开。 

代码总共分为三个模块：生成数据集 (generateds.py)、前向传播(forward.py)、反向传播 (backward.py)。

 ①生成数据集的模块(generateds.py) 

②前向传播模块(forward.py) 

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_21.png" />

③反向传播模块(backward.py) 

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_22.png" />

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_23.png" />

运行代码，结果如下： 

<img src="{{ site.url }}/assets//blog_images/ML/机器学习概念_24.png" />

由运行结果可见，程序使用模块化设计方法，加入指数衰减学习率，使用正则化后，红色点和蓝色点 的分割曲线相对平滑，效果变好。 



















