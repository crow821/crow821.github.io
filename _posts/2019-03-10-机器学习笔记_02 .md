---
title: æœºå™¨å­¦ä¹ ç¬”è®°_02
categories:
- ML
tags:
- tensorflow
updated: 2019-03-10
---



 

è§†é¢‘æ¥æºï¼šä¸­å›½æ…•è¯¾ç½‘ äººå·¥æ™ºèƒ½å®è·µï¼štensorflowç¬”è®°_ç¥ç»ç½‘ç»œä¼˜åŒ–

 âˆšç¥ç»å…ƒæ¨¡å‹ï¼šç”¨æ•°å­¦å…¬å¼è¡¨ç¤ºä¸ºï¼šğŸ(âˆ‘ğ’Šğ’™ğ’Šğ’˜ğ’Š + ğ›)ï¼Œf ä¸ºæ¿€æ´»å‡½æ•°ã€‚ç¥ç»ç½‘ç»œæ˜¯ä»¥ç¥ç»å…ƒä¸ºåŸºæœ¬å• å…ƒæ„æˆçš„ã€‚ 

âˆšæ¿€æ´»å‡½æ•°ï¼šå¼•å…¥éçº¿æ€§æ¿€æ´»å› ç´ ï¼Œæé«˜æ¨¡å‹çš„è¡¨è¾¾åŠ›ã€‚

 å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°æœ‰ reluã€sigmoidã€tanh ç­‰ã€‚ 

â‘  æ¿€æ´»å‡½æ•° relu: åœ¨ Tensorflow ä¸­ï¼Œç”¨ tf.nn.relu()è¡¨ç¤º  

<img src="{{ site.url }}/assets//blog_images/ML/æœºå™¨å­¦ä¹ æ¦‚å¿µ_07.png" />

â‘¡ æ¿€æ´»å‡½æ•° sigmoidï¼šåœ¨ Tensorflow ä¸­ï¼Œç”¨ tf.nn.sigmoid()è¡¨ç¤º  

<img src="{{ site.url }}/assets//blog_images/ML/æœºå™¨å­¦ä¹ æ¦‚å¿µ_08.png" />

â‘¢ æ¿€æ´»å‡½æ•° tanhï¼šåœ¨ Tensorflow ä¸­ï¼Œç”¨ tf.nn.tanh()è¡¨ç¤º  

<img src="{{ site.url }}/assets//blog_images/ML/æœºå™¨å­¦ä¹ æ¦‚å¿µ_09.png" />

âˆšç¥ç»ç½‘ç»œçš„å¤æ‚åº¦ï¼šå¯ç”¨ç¥ç»ç½‘ç»œçš„å±‚æ•°å’Œç¥ç»ç½‘ç»œä¸­å¾…ä¼˜åŒ–å‚æ•°ä¸ªæ•°è¡¨ç¤º

 âˆšç¥ç»ç½‘è·¯çš„å±‚æ•°ï¼šä¸€èˆ¬ä¸è®¡å…¥è¾“å…¥å±‚ï¼Œ**å±‚æ•° = n ä¸ªéšè—å±‚ + 1 ä¸ªè¾“å‡ºå±‚**  

 âˆšç¥ç»ç½‘è·¯å¾…ä¼˜åŒ–çš„å‚æ•°ï¼šç¥ç»ç½‘ç»œä¸­æ‰€æœ‰å‚æ•° w çš„ä¸ªæ•° + æ‰€æœ‰å‚æ•° b çš„ä¸ªæ•°  

<img src="{{ site.url }}/assets//blog_images/ML/æœºå™¨å­¦ä¹ æ¦‚å¿µ_10.png" />

âˆšæŸå¤±å‡½æ•°ï¼ˆlossï¼‰ï¼šç”¨æ¥è¡¨ç¤ºé¢„æµ‹å€¼ï¼ˆyï¼‰ä¸å·²çŸ¥ç­”æ¡ˆï¼ˆy_ï¼‰çš„å·®è·ã€‚åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œé€šè¿‡ä¸æ–­ æ”¹å˜ç¥ç»ç½‘ç»œä¸­æ‰€æœ‰å‚æ•°ï¼Œä½¿æŸå¤±å‡½æ•°ä¸æ–­å‡å°ï¼Œä»è€Œè®­ç»ƒå‡ºæ›´é«˜å‡†ç¡®ç‡çš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚ _

_âˆšå¸¸ç”¨çš„æŸå¤±å‡½æ•°æœ‰å‡æ–¹è¯¯å·®ã€è‡ªå®šä¹‰å’Œäº¤å‰ç†µç­‰ã€‚

 âˆšå‡æ–¹è¯¯å·® mseï¼šn ä¸ªæ ·æœ¬çš„é¢„æµ‹å€¼ y ä¸å·²çŸ¥ç­”æ¡ˆ y_ä¹‹å·®çš„å¹³æ–¹å’Œï¼Œå†æ±‚å¹³å‡å€¼ã€‚ 

<img src="{{ site.url }}/assets//blog_images/ML/æœºå™¨å­¦ä¹ æ¦‚å¿µ_11.png" />

âˆš**äº¤å‰ç†µ(Cross Entropy)ï¼šè¡¨ç¤ºä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ã€‚äº¤å‰ç†µè¶Šå¤§ï¼Œä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒè·ç¦»è¶Šè¿œï¼Œä¸¤ ä¸ªæ¦‚ç‡åˆ†å¸ƒè¶Šç›¸å¼‚ï¼›äº¤å‰ç†µè¶Šå°ï¼Œä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒè·ç¦»è¶Šè¿‘ï¼Œä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒè¶Šç›¸ä¼¼ã€‚** 

äº¤å‰ç†µè®¡ç®—å…¬å¼ï¼šğ‡(ğ²_ , ğ²) = âˆ’âˆ‘ğ²_ âˆ— ğ’ğ’ğ’ˆ ğ’š 

ç”¨ Tensorflow å‡½æ•°è¡¨ç¤ºä¸º 

**ce= -tf.reduce_mean(y_* tf.log(tf.clip_by_value(y, 1e-12, 1.0)))**  

ä¾‹å¦‚ï¼š

 ä¸¤ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹è§£å†³äºŒåˆ†ç±»é—®é¢˜ä¸­ï¼Œå·²çŸ¥æ ‡å‡†ç­”æ¡ˆä¸º y_ = (1, 0)ï¼Œç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹é¢„æµ‹ç»“æœä¸º 

y1=(0.6, 0.4)ï¼Œç¬¬äºŒä¸ªç¥ç»ç½‘ç»œæ¨¡å‹é¢„æµ‹ç»“æœä¸º y2=(0.8, 0.2)ï¼Œåˆ¤æ–­å“ªä¸ªç¥ç»ç½‘ç»œæ¨¡å‹é¢„æµ‹çš„ç»“æœæ›´æ¥ è¿‘æ ‡å‡†ç­”æ¡ˆã€‚ 

æ ¹æ®äº¤å‰ç†µçš„è®¡ç®—å…¬å¼å¾—ï¼š

 H1((1,0),(0.6,0.4)) = -(1*log0.6 + 0*log0.4) â‰ˆ -(-0.222 + 0) = 0.222

 H2((1,0),(0.8,0.2)) = -(1*log0.8 + 0*log0.2) â‰ˆ -(-0.097 + 0) = 0.097 

ç”±äº 0.222>0.097ï¼Œæ‰€ä»¥é¢„æµ‹ç»“æœ y2 ä¸æ ‡å‡†ç­”æ¡ˆ y_æ›´æ¥è¿‘ï¼Œy2 é¢„æµ‹æ›´å‡†ç¡® 

<img src="{{ site.url }}/assets//blog_images/ML/æœºå™¨å­¦ä¹ æ¦‚å¿µ_12.png" />

**softmax å‡½æ•°**åº”ç”¨ï¼š

åœ¨ n åˆ†ç±»ä¸­ï¼Œæ¨¡å‹ä¼šæœ‰ n ä¸ªè¾“å‡ºï¼Œå³ y1,y2â€¦ynï¼Œå…¶ä¸­ yi è¡¨ç¤ºç¬¬ i ç§æƒ…å†µå‡ºç°çš„å¯ èƒ½æ€§å¤§å°ã€‚å°† n ä¸ªè¾“å‡ºç»è¿‡ softmax å‡½æ•°ï¼Œå¯å¾—åˆ°ç¬¦åˆæ¦‚ç‡åˆ†å¸ƒçš„åˆ†ç±»ç»“æœã€‚

 âˆšåœ¨ Tensorflow ä¸­ï¼Œä¸€èˆ¬è®©æ¨¡å‹çš„è¾“å‡ºç»è¿‡ sofemax å‡½æ•°ï¼Œä»¥è·å¾—è¾“å‡ºåˆ†ç±»çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå†ä¸æ ‡å‡† ç­”æ¡ˆå¯¹æ¯”ï¼Œæ±‚å‡ºäº¤å‰ç†µï¼Œå¾—åˆ°æŸå¤±å‡½æ•°ï¼Œç”¨å¦‚ä¸‹å‡½æ•°å®ç°ï¼š

 **ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))** 

**cem = tf.reduce_mean(ce)** 

è®¡ç®—å‡ºçš„ç»“æœå°±æ˜¯å½“å‰è®¡ç®—å‡ºçš„é¢„æµ‹å€¼å’Œæ ‡å‡†å€¼ä¹‹é—´çš„å·®è·ï¼Œä¹Ÿå°±æ˜¯äº¤å‰ç†µ

**âˆšå­¦ä¹ ç‡** learning_rateï¼šè¡¨ç¤ºäº†æ¯æ¬¡å‚æ•°æ›´æ–°çš„å¹…åº¦å¤§å°ã€‚å­¦ä¹ ç‡è¿‡å¤§ï¼Œä¼šå¯¼è‡´å¾…ä¼˜åŒ–çš„å‚æ•°åœ¨æœ€ å°å€¼é™„è¿‘æ³¢åŠ¨ï¼Œä¸æ”¶æ•›ï¼›å­¦ä¹ ç‡è¿‡å°ï¼Œä¼šå¯¼è‡´å¾…ä¼˜åŒ–çš„å‚æ•°æ”¶æ•›ç¼“æ…¢ã€‚ 

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå‚æ•°çš„æ›´æ–°å‘ç€æŸå¤±å‡½æ•°æ¢¯åº¦ä¸‹é™çš„æ–¹å‘ã€‚ å‚æ•°çš„æ›´æ–°å…¬å¼ä¸ºï¼š

<img src="{{ site.url }}/assets//blog_images/ML/æœºå™¨å­¦ä¹ æ¦‚å¿µ_13.png" />
<img src="{{ site.url }}/assets//blog_images/ML/æœºå™¨å­¦ä¹ æ¦‚å¿µ_14.png" />

<img src="{{ site.url }}/assets//blog_images/ML/æœºå™¨å­¦ä¹ æ¦‚å¿µ_15.png" />

ä»£ç å¦‚ä¸‹ï¼š

```python
# coding: utf-8
# è®¾æŸå¤±å‡½æ•° loss=(w+1)^2,ä»¤wåˆå€¼æ˜¯å¸¸æ•°5ã€‚åå‘ä¼ æ’­å°±æ˜¯æ±‚æœ€ä¼˜wï¼Œå³æ±‚æœ€å°losså¯¹åº”çš„wå€¼
import tensorflow as tf
# å®šä¹‰å¾…ä¼˜åŒ–å‚æ•°wåˆå€¼èµ‹5
w = tf.Variable(tf.constant(5, dtype=tf.float32))
# å®šä¹‰æŸå¤±å‡½æ•°loss
loss = tf.square(w+1)
# å®šä¹‰åå‘ä¼ æ’­æ–¹æ³•
train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)
# ç”Ÿæˆä¼šè¯ï¼Œè®­ç»ƒ40è½®
with tf.Session() as sess:
    inin_op = tf.global_variables_initializer()
    sess.run(inin_op)
    for i in range(40):
        sess.run(train_step)
        w_val = sess.run(w)
        loss_val = sess.run(loss)
        print("After %s steps: w is %f, loss is %f." %(i, w_val, loss_val))
"""
F:\pycharmcache\venv\Scripts\python.exe F:/pycharmcache/tensorflowå­¦ä¹ /15_å­¦ä¹ ç‡.py
WARNING:tensorflow:From F:\pycharmcache\venv\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-03-10 20:20:33.971784: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
After 0 steps: w is 2.600000, loss is 12.959999.
After 1 steps: w is 1.160000, loss is 4.665599.
After 2 steps: w is 0.296000, loss is 1.679616.
After 3 steps: w is -0.222400, loss is 0.604662.
After 4 steps: w is -0.533440, loss is 0.217678.
After 5 steps: w is -0.720064, loss is 0.078364.
After 6 steps: w is -0.832038, loss is 0.028211.
After 7 steps: w is -0.899223, loss is 0.010156.
After 8 steps: w is -0.939534, loss is 0.003656.
After 9 steps: w is -0.963720, loss is 0.001316.
After 10 steps: w is -0.978232, loss is 0.000474.
After 11 steps: w is -0.986939, loss is 0.000171.
After 12 steps: w is -0.992164, loss is 0.000061.
After 13 steps: w is -0.995298, loss is 0.000022.
After 14 steps: w is -0.997179, loss is 0.000008.
After 15 steps: w is -0.998307, loss is 0.000003.
After 16 steps: w is -0.998984, loss is 0.000001.
After 17 steps: w is -0.999391, loss is 0.000000.
After 18 steps: w is -0.999634, loss is 0.000000.
After 19 steps: w is -0.999781, loss is 0.000000.
After 20 steps: w is -0.999868, loss is 0.000000.
After 21 steps: w is -0.999921, loss is 0.000000.
After 22 steps: w is -0.999953, loss is 0.000000.
After 23 steps: w is -0.999972, loss is 0.000000.
After 24 steps: w is -0.999983, loss is 0.000000.
After 25 steps: w is -0.999990, loss is 0.000000.
After 26 steps: w is -0.999994, loss is 0.000000.
After 27 steps: w is -0.999996, loss is 0.000000.
After 28 steps: w is -0.999998, loss is 0.000000.
After 29 steps: w is -0.999999, loss is 0.000000.
After 30 steps: w is -0.999999, loss is 0.000000.
After 31 steps: w is -1.000000, loss is 0.000000.
After 32 steps: w is -1.000000, loss is 0.000000.
After 33 steps: w is -1.000000, loss is 0.000000.
After 34 steps: w is -1.000000, loss is 0.000000.
After 35 steps: w is -1.000000, loss is 0.000000.
After 36 steps: w is -1.000000, loss is 0.000000.
After 37 steps: w is -1.000000, loss is 0.000000.
After 38 steps: w is -1.000000, loss is 0.000000.
After 39 steps: w is -1.000000, loss is 0.000000.

Process finished with exit code 0

"""
```

ç”±å›¾å¯çŸ¥ï¼ŒæŸå¤±å‡½æ•° loss çš„æœ€å°å€¼ä¼šåœ¨(-1,0)å¤„å¾—åˆ°ï¼Œæ­¤æ—¶æŸå¤±å‡½æ•°çš„å¯¼æ•°ä¸º 0,å¾—åˆ°æœ€ç»ˆå‚æ•° w = -1ã€‚ 

**âˆšå­¦ä¹ ç‡çš„è®¾ç½®** 

å­¦ä¹ ç‡è¿‡å¤§ï¼Œä¼šå¯¼è‡´å¾…ä¼˜åŒ–çš„å‚æ•°åœ¨æœ€å°å€¼é™„è¿‘æ³¢åŠ¨ï¼Œä¸æ”¶æ•›ï¼›å­¦ä¹ ç‡è¿‡å°ï¼Œä¼šå¯¼è‡´å¾…ä¼˜åŒ–çš„å‚æ•°æ”¶ æ•›ç¼“æ…¢ 

ä¾‹å¦‚ï¼š

 â‘  å¯¹äºä¸Šä¾‹çš„æŸå¤±å‡½æ•° loss = (w + 1)2ã€‚

åˆ™å°†ä¸Šè¿°ä»£ç ä¸­å­¦ä¹ ç‡ä¿®æ”¹ä¸º 1ï¼Œå…¶ä½™å†…å®¹ä¸å˜ã€‚ å®éªŒç»“æœå¦‚ä¸‹ï¼š 

```python 
train_step = tf.train.GradientDescentOptimizer(1).minimize(loss)
```



```python
"""
F:\pycharmcache\venv\Scripts\python.exe F:/pycharmcache/tensorflowå­¦ä¹ /15_å­¦ä¹ ç‡.py
WARNING:tensorflow:From F:\pycharmcache\venv\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-03-10 20:24:36.066876: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
After 0 steps: w is -7.000000, loss is 36.000000.
After 1 steps: w is 5.000000, loss is 36.000000.
After 2 steps: w is -7.000000, loss is 36.000000.
After 3 steps: w is 5.000000, loss is 36.000000.
After 4 steps: w is -7.000000, loss is 36.000000.
After 5 steps: w is 5.000000, loss is 36.000000.
After 6 steps: w is -7.000000, loss is 36.000000.
After 7 steps: w is 5.000000, loss is 36.000000.
After 8 steps: w is -7.000000, loss is 36.000000.
After 9 steps: w is 5.000000, loss is 36.000000.
After 10 steps: w is -7.000000, loss is 36.000000.
After 11 steps: w is 5.000000, loss is 36.000000.
After 12 steps: w is -7.000000, loss is 36.000000.
After 13 steps: w is 5.000000, loss is 36.000000.
After 14 steps: w is -7.000000, loss is 36.000000.
After 15 steps: w is 5.000000, loss is 36.000000.
After 16 steps: w is -7.000000, loss is 36.000000.
After 17 steps: w is 5.000000, loss is 36.000000.
After 18 steps: w is -7.000000, loss is 36.000000.
After 19 steps: w is 5.000000, loss is 36.000000.
After 20 steps: w is -7.000000, loss is 36.000000.
After 21 steps: w is 5.000000, loss is 36.000000.
After 22 steps: w is -7.000000, loss is 36.000000.
After 23 steps: w is 5.000000, loss is 36.000000.
After 24 steps: w is -7.000000, loss is 36.000000.
After 25 steps: w is 5.000000, loss is 36.000000.
After 26 steps: w is -7.000000, loss is 36.000000.
After 27 steps: w is 5.000000, loss is 36.000000.
After 28 steps: w is -7.000000, loss is 36.000000.
After 29 steps: w is 5.000000, loss is 36.000000.
After 30 steps: w is -7.000000, loss is 36.000000.
After 31 steps: w is 5.000000, loss is 36.000000.
After 32 steps: w is -7.000000, loss is 36.000000.
After 33 steps: w is 5.000000, loss is 36.000000.
After 34 steps: w is -7.000000, loss is 36.000000.
After 35 steps: w is 5.000000, loss is 36.000000.
After 36 steps: w is -7.000000, loss is 36.000000.
After 37 steps: w is 5.000000, loss is 36.000000.
After 38 steps: w is -7.000000, loss is 36.000000.
After 39 steps: w is 5.000000, loss is 36.000000.

Process finished with exit code 0
"""

```

åˆ™å°†ä¸Šè¿°ä»£ç ä¸­å­¦ä¹ ç‡ä¿®æ”¹ä¸º 0.001ï¼Œå…¶ä½™å†…å®¹ä¸å˜ã€‚ å®éªŒç»“æœå¦‚ä¸‹ï¼š

```python
train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)
```

```python
"""
F:\pycharmcache\venv\Scripts\python.exe F:/pycharmcache/tensorflowå­¦ä¹ /15_å­¦ä¹ ç‡.py
WARNING:tensorflow:From F:\pycharmcache\venv\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-03-10 20:27:20.014024: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
After 0 steps: w is 4.988000, loss is 35.856144.
After 1 steps: w is 4.976024, loss is 35.712864.
After 2 steps: w is 4.964072, loss is 35.570156.
After 3 steps: w is 4.952144, loss is 35.428020.
After 4 steps: w is 4.940240, loss is 35.286449.
After 5 steps: w is 4.928360, loss is 35.145447.
After 6 steps: w is 4.916503, loss is 35.005009.
After 7 steps: w is 4.904670, loss is 34.865124.
After 8 steps: w is 4.892860, loss is 34.725803.
After 9 steps: w is 4.881075, loss is 34.587044.
After 10 steps: w is 4.869313, loss is 34.448833.
After 11 steps: w is 4.857574, loss is 34.311172.
After 12 steps: w is 4.845859, loss is 34.174068.
After 13 steps: w is 4.834167, loss is 34.037510.
After 14 steps: w is 4.822499, loss is 33.901497.
After 15 steps: w is 4.810854, loss is 33.766029.
After 16 steps: w is 4.799233, loss is 33.631104.
After 17 steps: w is 4.787634, loss is 33.496712.
After 18 steps: w is 4.776059, loss is 33.362858.
After 19 steps: w is 4.764507, loss is 33.229538.
After 20 steps: w is 4.752978, loss is 33.096756.
After 21 steps: w is 4.741472, loss is 32.964497.
After 22 steps: w is 4.729989, loss is 32.832775.
After 23 steps: w is 4.718529, loss is 32.701576.
After 24 steps: w is 4.707092, loss is 32.570904.
After 25 steps: w is 4.695678, loss is 32.440750.
After 26 steps: w is 4.684287, loss is 32.311119.
After 27 steps: w is 4.672918, loss is 32.182003.
After 28 steps: w is 4.661572, loss is 32.053402.
After 29 steps: w is 4.650249, loss is 31.925320.
After 30 steps: w is 4.638949, loss is 31.797745.
After 31 steps: w is 4.627671, loss is 31.670683.
After 32 steps: w is 4.616416, loss is 31.544128.
After 33 steps: w is 4.605183, loss is 31.418077.
After 34 steps: w is 4.593973, loss is 31.292530.
After 35 steps: w is 4.582785, loss is 31.167484.
After 36 steps: w is 4.571619, loss is 31.042938.
After 37 steps: w is 4.560476, loss is 30.918892.
After 38 steps: w is 4.549355, loss is 30.795341.
After 39 steps: w is 4.538256, loss is 30.672281.

Process finished with exit code 0

"""
```

ç”±è¿è¡Œç»“æœå¯çŸ¥ï¼ŒæŸå¤±å‡½æ•° loss å€¼ç¼“æ…¢ä¸‹é™ï¼Œw å€¼ä¹Ÿåœ¨å°å¹…åº¦å˜åŒ–ï¼Œæ”¶æ•›ç¼“æ…¢ã€‚ å¤§äº†æˆ–è€…æ˜¯å°äº†éƒ½ä¼šæœ‰å½±å“ã€‚

**âˆšæŒ‡æ•°è¡°å‡å­¦ä¹ ç‡ï¼šå­¦ä¹ ç‡éšç€è®­ç»ƒè½®æ•°å˜åŒ–è€ŒåŠ¨æ€æ›´æ–°** 

**å­¦ä¹ ç‡è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š** 

<img src="{{ site.url }}/assets//blog_images/ML/æœºå™¨å­¦ä¹ æ¦‚å¿µ_16.png" />

å…¶ä¸­ï¼ŒLEARNING_RATE_BASE ä¸ºå­¦ä¹ ç‡åˆå§‹å€¼

LEARNING_RATE_DECAY ä¸ºå­¦ä¹ ç‡è¡°å‡ç‡

global_step è®° å½•äº†å½“å‰è®­ç»ƒè½®æ•°ï¼Œä¸ºä¸å¯è®­ç»ƒå‹å‚æ•°(0,trainable = False)

å­¦ä¹ ç‡ learning_rate æ›´æ–°é¢‘ç‡ä¸ºè¾“å…¥æ•°æ®é›†æ€»æ ·æœ¬æ•°é™¤ä»¥æ¯ æ¬¡å–‚å…¥æ ·æœ¬æ•°

è‹¥ staircase è®¾ç½®ä¸º True æ—¶ï¼Œè¡¨ç¤º global_step/learning rate step å–æ•´æ•°ï¼Œå­¦ä¹ ç‡é˜¶æ¢¯å‹è¡°å‡ï¼›

è‹¥ staircase è®¾ç½®ä¸º false æ—¶ï¼Œå­¦ä¹ ç‡ä¼šæ˜¯ä¸€æ¡å¹³æ»‘ä¸‹é™çš„æ›²çº¿ 

åœ¨æœ¬ä¾‹ä¸­ï¼Œæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸è®¾å®šå›ºå®šçš„å­¦ä¹ ç‡ï¼Œä½¿ç”¨æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡è¿›è¡Œè®­ç»ƒã€‚å…¶ä¸­ï¼Œå­¦ä¹ ç‡åˆå€¼è®¾ ç½®ä¸º 0.1ï¼Œå­¦ä¹ ç‡è¡°å‡ç‡è®¾ç½®ä¸º 0.99ï¼ŒBATCH_SIZE è®¾ç½®ä¸º 1ã€‚ 

ä»£ç å¦‚ä¸‹ï¼š 

```python 
# coding: utf-8
# coding: utf-8
# è®¾æŸå¤±å‡½æ•° loss=(w+1)^2,ä»¤wåˆå€¼æ˜¯å¸¸æ•°10ã€‚åå‘ä¼ æ’­å°±æ˜¯æ±‚æœ€ä¼˜wï¼Œå³æ±‚æœ€å°losså¯¹åº”çš„wå€¼
# ä½¿ç”¨æŒ‡æ•°è¡°å‡çš„å­¦ä¹ ç‡ï¼Œåœ¨è¿­ä»£åˆæœŸå¾—åˆ°è¾ƒé«˜çš„ä¸‹é™é€Ÿåº¦ï¼Œå¯ä»¥åœ¨è¾ƒå°çš„è®­ç»ƒè½®æ•°ä¸‹å»çš„æ›´æœ‰æ”¶æ•›åº¦
import tensorflow as tf
LEARNING_RATE_BASE = 0.1 # æœ€åˆçš„å­¦ä¹ ç‡
LEARNING_RATE_DECAY = 0.99 # å­¦ä¹ ç‡è¡°å‡ç‡
LEARNING_RATE_STEP = 1 # å–‚å…¥å¤šå°‘è½®BATCH_SIZEåï¼Œæ›´æ–°ä¸€æ¬¡å­¦ä¹ ç‡ï¼Œä¸€èˆ¬è®¾ç½®ä¸ºï¼šæ€»æ ·æœ¬æ•°/BATCH_SIZE
# è¿è¡Œäº†å‡ è½®BATCH_SIZEçš„è®¡æ•°å™¨ï¼Œåˆå§‹ç»™0ï¼Œè®¾ä¸ºä¸è¢«è®­ç»ƒ
global_step = tf.Variable(0, trainable=False)
# å®šä¹‰æŒ‡æ•°ä¸‹é™å­¦ä¹ ç‡
learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, LEARNING_RATE_STEP,
                                           LEARNING_RATE_DECAY, staircase=True)
# å®šä¹‰å¾…ä¼˜åŒ–å‚æ•°ï¼Œåˆå€¼ç»™10
w = tf.Variable(tf.constant(10, dtype=tf.float32))
# å®šä¹‰æŸå¤±å‡½æ•°loss
loss = tf.square(w+1)
# å®šä¹‰åå‘ä¼ æ’­æ–¹æ³•
train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=
                                                                       global_step)
# ç”Ÿæˆä¼šè¯ï¼Œè®­ç»ƒ40è½®
with tf.Session() as sess:
    inin_op = tf.global_variables_initializer()
    sess.run(inin_op)
    for i in range(40):
        sess.run(train_step)
        w_val = sess.run(w)
        loss_val = sess.run(loss)
        print("After %s steps: w is %f, loss is %f." %(i, w_val, loss_val))
"""
F:\pycharmcache\venv\Scripts\python.exe F:/pycharmcache/tensorflowå­¦ä¹ /16_åŠ å…¥æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡.py
WARNING:tensorflow:From F:\pycharmcache\venv\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-03-10 21:01:38.994750: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
After 0 steps: w is 7.800000, loss is 77.440002.
After 1 steps: w is 6.057600, loss is 49.809719.
After 2 steps: w is 4.674169, loss is 32.196194.
After 3 steps: w is 3.573041, loss is 20.912704.
After 4 steps: w is 2.694472, loss is 13.649122.
After 5 steps: w is 1.991791, loss is 8.950810.
After 6 steps: w is 1.428448, loss is 5.897361.
After 7 steps: w is 0.975754, loss is 3.903603.
After 8 steps: w is 0.611130, loss is 2.595741.
After 9 steps: w is 0.316771, loss is 1.733887.
After 10 steps: w is 0.078598, loss is 1.163374.
After 11 steps: w is -0.114544, loss is 0.784033.
After 12 steps: w is -0.271515, loss is 0.530691.
After 13 steps: w is -0.399367, loss is 0.360760.
After 14 steps: w is -0.503727, loss is 0.246287.
After 15 steps: w is -0.589091, loss is 0.168846.
After 16 steps: w is -0.659066, loss is 0.116236.
After 17 steps: w is -0.716543, loss is 0.080348.
After 18 steps: w is -0.763853, loss is 0.055765.
After 19 steps: w is -0.802872, loss is 0.038859.
After 20 steps: w is -0.835119, loss is 0.027186.
After 21 steps: w is -0.861821, loss is 0.019094.
After 22 steps: w is -0.883974, loss is 0.013462.
After 23 steps: w is -0.902390, loss is 0.009528.
After 24 steps: w is -0.917728, loss is 0.006769.
After 25 steps: w is -0.930527, loss is 0.004827.
After 26 steps: w is -0.941226, loss is 0.003454.
After 27 steps: w is -0.950187, loss is 0.002481.
After 28 steps: w is -0.957706, loss is 0.001789.
After 29 steps: w is -0.964026, loss is 0.001294.
After 30 steps: w is -0.969348, loss is 0.000940.
After 31 steps: w is -0.973838, loss is 0.000684.
After 32 steps: w is -0.977631, loss is 0.000500.
After 33 steps: w is -0.980842, loss is 0.000367.
After 34 steps: w is -0.983565, loss is 0.000270.
After 35 steps: w is -0.985877, loss is 0.000199.
After 36 steps: w is -0.987844, loss is 0.000148.
After 37 steps: w is -0.989520, loss is 0.000110.
After 38 steps: w is -0.990951, loss is 0.000082.
After 39 steps: w is -0.992174, loss is 0.000061.

Process finished with exit code 0

"""
```

ç»“æœå¯ä»¥çœ‹å‡ºï¼Œéšç€è®­ç»ƒè½®æ•°å¢åŠ å­¦ä¹ ç‡åœ¨ä¸æ–­å‡å°ã€‚ 

**âˆšæ»‘åŠ¨å¹³å‡ï¼šè®°å½•äº†ä¸€æ®µæ—¶é—´å†…æ¨¡å‹ä¸­æ‰€æœ‰å‚æ•° w å’Œ b å„è‡ªçš„å¹³å‡å€¼ã€‚åˆ©ç”¨æ»‘åŠ¨å¹³å‡å€¼å¯ä»¥å¢å¼ºæ¨¡ å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚** 
é’ˆå¯¹æ‰€æœ‰å‚æ•°ï¼šwå’Œb

ï¼ˆå°±åƒæ˜¯ç»™å‚æ•°åŠ äº†ä¸€ä¸ªå½±å­ï¼Œå‚æ•°å˜åŒ–ï¼Œå½±å­ç¼“æ…¢è¿½éšï¼‰

âˆšæ»‘åŠ¨å¹³å‡å€¼ï¼ˆå½±å­ï¼‰è®¡ç®—å…¬å¼ï¼š å½±å­ = è¡°å‡ç‡ * å½±å­ +ï¼ˆ1 - è¡°å‡ç‡ï¼‰* å‚æ•°  

<img src="{{ site.url }}/assets//blog_images/ML/æœºå™¨å­¦ä¹ æ¦‚å¿µ_17.png" />

âˆšç”¨ Tesnsorflow å‡½æ•°è¡¨ç¤ºä¸ºï¼š

 **âˆšema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAYï¼Œglobal_step)** 

å…¶ä¸­ï¼ŒMOVING_AVERAGE_DECAY è¡¨ç¤ºæ»‘åŠ¨å¹³å‡è¡°å‡ç‡ï¼Œä¸€èˆ¬ä¼šèµ‹æ¥è¿‘ 1 çš„å€¼ï¼Œglobal_step è¡¨ç¤ºå½“å‰ è®­ç»ƒäº†å¤šå°‘è½®ã€‚

 **âˆšema_op = ema.apply(tf.trainable_variables())**

 å…¶ä¸­ï¼Œema.apply()å‡½æ•°å®ç°å¯¹æ‹¬å·å†…å‚æ•°æ±‚æ»‘åŠ¨å¹³å‡ï¼Œtf.trainable_variables()å‡½æ•°å®ç°æŠŠæ‰€æœ‰ å¾…è®­ç»ƒå‚æ•°æ±‡æ€»ä¸ºåˆ—è¡¨ã€‚

 **âˆšwith tf.control_dependencies([train_step, ema_op]): train_op =                                                         	               ************tf.no_op(name='train')** 

å…¶ä¸­ï¼Œè¯¥å‡½æ•°å®ç°å°†æ»‘åŠ¨å¹³å‡å’Œè®­ç»ƒè¿‡ç¨‹åŒæ­¥è¿è¡Œã€‚

 æŸ¥çœ‹æ¨¡å‹ä¸­å‚æ•°çš„å¹³å‡å€¼ï¼Œå¯ä»¥ç”¨ ema.average()å‡½æ•°ã€‚ 

ä¾‹å¦‚ï¼š

 åœ¨ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­ï¼Œå°† MOVING_AVERAGE_DECAY è®¾ç½®ä¸º 0.99ï¼Œå‚æ•° w1 è®¾ç½®ä¸º 0ï¼Œw1 çš„æ»‘åŠ¨å¹³å‡å€¼è®¾ ç½®ä¸º 0ã€‚

 â‘ å¼€å§‹æ—¶ï¼Œè½®æ•° global_step è®¾ç½®ä¸º 0ï¼Œå‚æ•° w1 æ›´æ–°ä¸º 1ï¼Œåˆ™ w1 çš„æ»‘åŠ¨å¹³å‡å€¼ä¸ºï¼š w1 æ»‘åŠ¨å¹³å‡å€¼=min(0.99,1/10)*0+(1â€“ min(0.99,1/10)*1 = 0.9

 2. å½“è½®æ•° global_step è®¾ç½®ä¸º 100 æ—¶ï¼Œå‚æ•° w1 æ›´æ–°ä¸º 10ï¼Œä»¥ä¸‹ä»£ç  global_step ä¿æŒä¸º 100ï¼Œæ¯ æ¬¡æ‰§è¡Œæ»‘åŠ¨å¹³å‡æ“ä½œå½±å­å€¼æ›´æ–°ï¼Œåˆ™æ»‘åŠ¨å¹³å‡å€¼å˜ä¸ºï¼š 

w1 æ»‘åŠ¨å¹³å‡å€¼=min(0.99,101/110)*0.9+(1â€“ min(0.99,101/110)*10 = 0.826+0.818=1.644

 â‘¢å†æ¬¡è¿è¡Œï¼Œå‚æ•° w1 æ›´æ–°ä¸º 1.644ï¼Œåˆ™æ»‘åŠ¨å¹³å‡å€¼å˜ä¸ºï¼š

 w1 æ»‘åŠ¨å¹³å‡å€¼=min(0.99,101/110)*1.644+(1â€“ min(0.99,101/110)*10 = 2.328 

â‘£å†æ¬¡è¿è¡Œï¼Œå‚æ•° w1 æ›´æ–°ä¸º 2.328ï¼Œåˆ™æ»‘åŠ¨å¹³å‡å€¼ï¼š w1 æ»‘åŠ¨å¹³å‡å€¼=2.956

 ä»£ç å¦‚ä¸‹ï¼š 

```python
# coding: utf-8
import tensorflow as tf

# 1. å®šä¹‰å˜é‡åŠæ»‘åŠ¨å¹³å‡ç±»
# å®šä¹‰ä¸€ä¸ª32ä½æµ®ç‚¹å˜é‡ï¼Œåˆå§‹å€¼ä½0.0ï¼Œè¿™ä¸ªä»£ç å°±æ˜¯ä¸æ–­æ›´æ–°w1å‚æ•°ï¼Œä¼˜åŒ–w1å‚æ•°ï¼Œæ»‘åŠ¨å¹³å‡åšäº†>ä¸ªw1çš„å½±å­
w1 = tf.Variable(0, dtype=tf.float32)
# å®šä¹‰num_updates(NNçš„è¿­ä»£è½®æ•°),åˆå§‹å€¼ä¸º0ï¼Œä¸å¯è¢«ä¼˜åŒ–ï¼ˆè®­ç»ƒï¼‰ï¼Œè¿™ä¸ªå‚æ•°ä¸è®­ç»ƒ
global_step = tf.Variable(0, trainable=False)
# å®ä¾‹åŒ–æ»‘åŠ¨å¹³å‡ç±»ï¼Œç»™åˆ å‡ç‡ä¸º0.99ï¼Œå½“å‰è½®æ•°global_step
MOVING_AVERAGE_DECAY = 0.99
ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)
# ema.apply åçš„æ‹¬å·é‡Œæ˜¯æ›´æ–°åˆ—è¡¨ï¼Œæ¯æ¬¡è¿è¡Œsess.run(ema_op)æ—¶ï¼Œå¯¹æ›´æ–°åˆ—è¡¨ä¸­çš„å…ƒç´ æ±‚æ»‘åŠ¨å¹³å‡å€¼
# åœ¨å®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨tf.trainable_variables()è‡ªåŠ¨å°†æ‰€æœ‰å¾…è®­ç»ƒçš„å‚æ•°æ±‡æ€»ä¸ºåˆ—è¡¨
# ema_op = ema.apply([w1])
ema_op = ema.apply(tf.trainable_variables())

# 2.æŸ¥çœ‹ä¸åŒè¿­ä»£ä¸­å˜é‡å–å€¼çš„å˜åŒ–
with tf.Session() as sess:
    # åˆå§‹åŒ–
    init_op = tf.global_variables_initializer()
    sess.run(init_op)
    # ç”¨ema.average(w1)è·å–w1æ»‘åŠ¨å¹³å‡å€¼ (è¦è¿è¡Œå¤šä¸ªèŠ‚ç‚¹ï¼Œä½œä¸ºåˆ—è¡¨ä¸­çš„å…ƒç´ åˆ—å‡ºï¼Œå†™åœ¨sess.runä¸­ï¼‰
    # æ‰“å°å‡ºå½“å‰å‚æ•°w1å’Œw1æ»‘åŠ¨å¹³å‡å€¼
    print(sess.run([w1, ema.average(w1)]))
    # å‚æ•°w1çš„å€¼èµ‹ä¸º1
    sess.run(tf.assign(w1, 1))
    sess.run(ema_op)
    print(sess.run([w1, ema.average(w1)]))

    # æ›´æ–°stepå’Œw1çš„å€¼ï¼Œæ¨¡æ‹Ÿå‡º100è½®è¿­ä»£åï¼Œå‚æ•°w1å˜ä¸º10
    sess.run(tf.assign(global_step, 100))
    sess.run(tf.assign(w1, 10))
    sess.run(ema_op)
    print(sess.run([w1, ema.average(w1)]))

    # æ¯æ¬¡sess.runä¼šæ›´æ–°ä¸€æ¬¡w1çš„æ»‘åŠ¨å¹³å‡å€¼
    sess.run(ema_op)
    print(sess.run([w1, ema.average(w1)]))

    sess.run(ema_op)
    print(sess.run([w1, ema.average(w1)]))

    sess.run(ema_op)
    print(sess.run([w1, ema.average(w1)]))

    sess.run(ema_op)
    print(sess.run([w1, ema.average(w1)]))

    sess.run(ema_op)
    print(sess.run([w1, ema.average(w1)]))

"""
F:\pycharmcache\venv\Scripts\python.exe F:/pycharmcache/tensorflowå­¦ä¹ /17_åŠ å…¥æ»‘åŠ¨å¹³å‡ç±».py
WARNING:tensorflow:From F:\pycharmcache\venv\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-03-11 10:32:47.740841: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
[0.0, 0.0]
[1.0, 0.9]
[10.0, 1.6445453]
[10.0, 2.3281732]
[10.0, 2.955868]
[10.0, 3.532206]
[10.0, 4.061389]
[10.0, 4.547275]

Process finished with exit code 0

"""
```

ä»è¿è¡Œç»“æœå¯çŸ¥ï¼Œæœ€åˆå‚æ•° w1 å’Œæ»‘åŠ¨å¹³å‡å€¼éƒ½æ˜¯ 0ï¼›å‚æ•° w1 è®¾å®šä¸º 1 åï¼Œæ»‘åŠ¨å¹³å‡å€¼å˜ä¸º 0.9ï¼› å½“è¿­ä»£è½®æ•°æ›´æ–°ä¸º 100 è½®æ—¶ï¼Œå‚æ•° w1 æ›´æ–°ä¸º 10 åï¼Œæ»‘åŠ¨å¹³å‡å€¼å˜ä¸º 1.644ã€‚éšåæ¯æ‰§è¡Œä¸€æ¬¡ï¼Œå‚æ•° w1 çš„æ»‘åŠ¨å¹³å‡å€¼éƒ½å‘å‚æ•° w1 é è¿‘ã€‚å¯è§ï¼Œæ»‘åŠ¨å¹³å‡è¿½éšå‚æ•°çš„å˜åŒ–è€Œå˜åŒ–ã€‚ 

âˆšè¿‡æ‹Ÿåˆï¼š

ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡è¾ƒé«˜ï¼Œåœ¨æ–°çš„æ•°æ®è¿›è¡Œé¢„æµ‹æˆ–åˆ†ç±»æ—¶å‡†ç¡®ç‡è¾ƒ ä½ï¼Œè¯´æ˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å·®ã€‚

 âˆš**æ­£åˆ™åŒ–**ï¼šåœ¨æŸå¤±å‡½æ•°ä¸­ç»™æ¯ä¸ªå‚æ•° w åŠ ä¸Šæƒé‡ï¼Œå¼•å…¥æ¨¡å‹å¤æ‚åº¦æŒ‡æ ‡ï¼Œä»è€ŒæŠ‘åˆ¶æ¨¡å‹å™ªå£°ï¼Œå‡å° è¿‡æ‹Ÿåˆã€‚(ä¸€èˆ¬ä¸æ­£åˆ™åŒ–b)

 ä½¿ç”¨æ­£åˆ™åŒ–åï¼ŒæŸå¤±å‡½æ•° loss å˜ä¸ºä¸¤é¡¹ä¹‹å’Œï¼š

 loss = loss(y ä¸ y_) + REGULARIZER*loss(w) 

å…¶ä¸­ï¼Œç¬¬ä¸€é¡¹æ˜¯é¢„æµ‹ç»“æœä¸æ ‡å‡†ç­”æ¡ˆä¹‹é—´çš„å·®è·ï¼Œå¦‚ä¹‹å‰è®²è¿‡çš„äº¤å‰ç†µã€å‡æ–¹è¯¯å·®ç­‰ï¼›ç¬¬äºŒé¡¹æ˜¯æ­£åˆ™ åŒ–è®¡ç®—ç»“æœã€‚ 

âˆšæ­£åˆ™åŒ–è®¡ç®—æ–¹æ³•ï¼š

 â‘  L1 æ­£åˆ™åŒ–ï¼š ğ’ğ’ğ’”ğ’”ğ‘³ğŸ = âˆ‘ğ’Š |ğ’˜ğ’Š |

 ç”¨ Tesnsorflow å‡½æ•°è¡¨ç¤º:loss(w) = tf.contrib.layers.l1_regularizer(REGULARIZER)(w) â‘¡ L2 æ­£åˆ™åŒ–ï¼š ğ’ğ’ğ’”ğ’”ğ‘³ğŸ = âˆ‘ğ’Š |ğ’˜ğ’Š | ğŸ

 ç”¨ Tesnsorflow å‡½æ•°è¡¨ç¤º:loss(w) = tf.contrib.layers.l2_regularizer(REGULARIZER)(w) âˆšç”¨ Tesnsorflow å‡½æ•°å®ç°æ­£åˆ™åŒ–ï¼š

 tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w)

 loss = cem + tf.add_n(tf.get_collection('losses')) 

ä¾‹å¦‚ï¼š

 ç”¨ 300 ä¸ªç¬¦åˆæ­£æ€åˆ†å¸ƒçš„ç‚¹ X[x0, x1]ä½œä¸ºæ•°æ®é›†ï¼Œæ ¹æ®ç‚¹ X[x0, x1]è®¡ç®—ç”Ÿæˆæ ‡æ³¨ Y_ï¼Œå°†æ•°æ®é›† æ ‡æ³¨ä¸ºçº¢è‰²ç‚¹å’Œè“è‰²ç‚¹ã€‚_

_ æ ‡æ³¨è§„åˆ™ä¸ºï¼šå½“ x0 2 + x1 2 < 2 æ—¶ï¼Œy_=1ï¼Œæ ‡æ³¨ä¸ºçº¢è‰²ï¼›å½“ x0 2 + x1 2 â‰¥2 æ—¶ï¼Œy_=0ï¼Œæ ‡æ³¨ä¸ºè“è‰²ã€‚ æˆ‘ä»¬åˆ†åˆ«ç”¨æ— æ­£åˆ™åŒ–å’Œæœ‰æ­£åˆ™åŒ–ä¸¤ç§æ–¹æ³•ï¼Œæ‹Ÿåˆæ›²çº¿ï¼ŒæŠŠçº¢è‰²ç‚¹å’Œè“è‰²ç‚¹åˆ†å¼€ã€‚åœ¨å®é™…åˆ†ç±»æ—¶ï¼Œ å¦‚æœå‰å‘ä¼ æ’­è¾“å‡ºçš„é¢„æµ‹å€¼ y æ¥è¿‘ 1 åˆ™ä¸ºçº¢è‰²ç‚¹æ¦‚ç‡è¶Šå¤§ï¼Œæ¥è¿‘ 0 åˆ™ä¸ºè“è‰²ç‚¹æ¦‚ç‡è¶Šå¤§ï¼Œè¾“å‡ºçš„é¢„ æµ‹å€¼ y ä¸º 0.5 æ˜¯çº¢è“ç‚¹æ¦‚ç‡åˆ†ç•Œçº¿ã€‚

 åœ¨æœ¬ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¹‹å‰æœªç”¨è¿‡çš„æ¨¡å—ä¸å‡½æ•°ï¼š

 âˆšmatplotlib æ¨¡å—ï¼šPython ä¸­çš„å¯è§†åŒ–å·¥å…·æ¨¡å—ï¼Œå®ç°å‡½æ•°å¯è§†åŒ– 

ç»ˆç«¯å®‰è£…æŒ‡ä»¤ï¼šsudo pip install matplotlib

 âˆšå‡½æ•° plt.scatterï¼ˆï¼‰ï¼šåˆ©ç”¨æŒ‡å®šé¢œè‰²å®ç°ç‚¹(x,y)çš„å¯è§†åŒ– 

plt.scatter (x åæ ‡, y åæ ‡, c=â€é¢œè‰²â€) 

plt.show()  

 âˆšæ”¶é›†è§„å®šåŒºåŸŸå†…æ‰€æœ‰çš„ç½‘æ ¼åæ ‡ç‚¹ï¼š

 xx, yy = np.mgrid[èµ·:æ­¢:æ­¥é•¿, èµ·:æ­¢:æ­¥é•¿] #æ‰¾åˆ°è§„å®šåŒºåŸŸä»¥æ­¥é•¿ä¸ºåˆ†è¾¨ç‡çš„è¡Œåˆ—ç½‘æ ¼åæ ‡ç‚¹

 grid = np.c_[xx.ravel(), yy.ravel()] #æ”¶é›†è§„å®šåŒºåŸŸå†…æ‰€æœ‰çš„ç½‘æ ¼åæ ‡ç‚¹

 âˆšplt.contour()å‡½æ•°ï¼šå‘ŠçŸ¥ xã€y åæ ‡å’Œå„ç‚¹é«˜åº¦ï¼Œç”¨ levels æŒ‡å®šé«˜åº¦çš„ç‚¹æä¸Šé¢œè‰² plt.contour (x è½´åæ ‡å€¼, y è½´åæ ‡å€¼, è¯¥ç‚¹çš„é«˜åº¦, levels=[ç­‰é«˜çº¿çš„é«˜åº¦]) 

plt.show()

 æœ¬ä¾‹ä»£ç å¦‚ä¸‹ï¼š

```python
# coding: utf-8
# 0å¯¼å…¥æ¨¡å—ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®é›†
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
BATCH_SIZE= 30
seed = 2
# åŸºäºseedäº§ç”Ÿéšæœºæ•°
rdm = np.random.RandomState(seed)
# éšæœºæ•°è¿”å›300è¡Œ2åˆ—çš„çŸ©é˜µï¼Œè¡¨ç¤º300ç»„åæ ‡ç‚¹ï¼ˆx0,x1)ä½œä¸ºè¾“å…¥æ•°æ®é›†
X = rdm.randn(300,2)
# ä»Xè¿™ä¸ª300è¡Œ2åˆ—çš„çŸ©é˜µä¸­å–å‡ºä¸€è¡Œï¼Œåˆ¤æ–­å¦‚æœä¸¤ä¸ªåæ ‡çš„å¹³æ–¹å’Œå°äº2ï¼Œç»™Yèµ‹å€¼1ï¼Œå…¶ä½™èµ‹å€¼0
# ä½œä¸ºè¾“å…¥æ•°æ®é›†çš„æ ‡ç­¾ï¼ˆæ­£ç¡®ç­”æ¡ˆï¼‰
Y_ = [int(x0*x0 + x1*x1 < 2) for (x0, x1) in X]
# éå†Yä¸­çš„æ¯ä¸ªå…ƒç´ ï¼Œ1èµ‹å€¼'red'å…¶ä½™èµ‹å€¼'blue',è¿™æ ·å¯è§†åŒ–æ˜¾ç¤ºæ—¶äººå¯ä»¥ç›´è§‚åŒºåˆ†
Y_c = [['red' if y else 'blue'] for y in Y_]
# å¯¹æ•°æ®é›†Xå’Œæ ‡ç­¾Yè¿›è¡Œshapeæ•´ç†ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ ä¸º-1è¡¨ç¤ºï¼Œéšç¬¬äºŒä¸ªå‚æ•°è®¡ç®—å¾—åˆ°ï¼Œç¬¬äºŒä¸ªå…ƒç´ è¡¨ç¤ºå¤šå°‘åˆ—ï¼ŒæŠŠXæ•´ç†ä¸ºn
# è¡Œ2åˆ—ï¼ŒæŠŠYæ•´ç†ä¸ºnè¡Œ1åˆ—
X = np.vstack(X).reshape(-1,2)
Y_ = np.vstack(Y_).reshape(-1,1)
print(X)
print(Y_)
print(Y_c)
# ç”¨plt.scatterç”»å‡ºæ•°æ®é›†Xå„è¡Œä¸­ç¬¬0åˆ—å…ƒç´ å’Œç¬¬1åˆ—å…ƒç´ çš„ç‚¹ï¼Œå³å„è¡Œçš„ï¼ˆx0,x1)ï¼Œç”¨å„è¡ŒY_cå¯¹åº”çš„å€¼è¡¨ç¤ºé¢œè‰²ï¼ˆ
# cæ˜¯colorçš„ç¼©å†™ï¼‰
plt.scatter(X[:, 0], X[:, 1], c=np.squeeze(Y_c))
plt.show()

# å®šä¹‰ç¥ç»ç½‘ç»œçš„è¾“å…¥ã€å‚æ•°å’Œè¾“å‡ºï¼Œå®šä¹‰å‰å‘ä¼ æ’­è¿‡ç¨‹


def get_weight(shape, regularizer):
    w = tf.Variable(tf.random_normal(shape), dtype=tf.float32)
    tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w))
    return w


def get_bias(shape):
    b = tf.Variable(tf.constant(0.01,shape=shape))
    return b


x = tf.placeholder(tf.float32, shape=(None, 2))
y_ = tf.placeholder(tf.float32, shape=(None, 1))
w1 = get_weight([2,11], 0.01)
b1 = get_bias([11])
y1 = tf.nn.relu(tf.matmul(x, w1) + b1)

w2 = get_weight([11,1],0.01)
b2 = get_bias([1])
y = tf.matmul(y1, w2) + b2 # è¾“å‡ºå±‚ä¸è¿‡æ¿€æ´»

# å®šä¹‰æŸå¤±å‡½æ•°
loos_mse = tf.reduce_mean(tf.square(y - y_))
loss_total = loos_mse + tf.add_n(tf.get_collection('losses'))
# å®šä¹‰åå‘ä¼ æ’­ç®—æ³•ï¼šä¸å«æ­£åˆ™åŒ–
train_step = tf.train.AdamOptimizer(0.0001).minimize(loos_mse)

with tf.Session() as sess:
    init_op = tf.global_variables_initializer()
    sess.run(init_op)
    STEPS = 40000
    for i in range(STEPS):
        start = (i * BATCH_SIZE) % 300
        end = start + BATCH_SIZE
        sess.run(train_step, feed_dict={x:X[start:end], y_:Y_[start:end]})
        if i % 2000 == 0:
            loos_mse_v = sess.run(loos_mse, feed_dict={x:X, y_:Y_})
            print("After %d steps, loss is: %f"%(i,loos_mse_v))
    # xxåœ¨-3åˆ°3ä¹‹é—´ä»¥æ­¥é•¿ä¸º0.01ï¼Œyyåœ¨-3åˆ°3ä¹‹é—´ä»¥æ­¥é•¿0.01ï¼Œç”ŸæˆäºŒç»´ç½‘ç»œåæ ‡ç‚¹
    xx, yy = np.mgrid[-3:3:.01, -3:3:.01]
    grid = np.c_[xx.ravel(), yy.ravel()]
    probs = sess.run(y, feed_dict={x:grid})
    probs = probs.reshape(xx.shape)
    print("w1:\n",sess.run(w1))
    print("b1:\n",sess.run(b1))
    print("w2:\n",sess.run(w2))
    print("b2:\n",sess.run(b2))
plt.scatter(X[:, 0], X[:, 1], c=np.squeeze(Y_c))
plt.contour(xx, yy, probs, levels=[.5])
plt.show()

"""
F:\pycharmcache\venv\Scripts\python.exe F:/pycharmcache/tensorflowå­¦ä¹ /18_åŠ å…¥æ­£åˆ™åŒ–.py
[[-4.16757847e-01 -5.62668272e-02]
 [-2.13619610e+00  1.64027081e+00]
 [-1.79343559e+00 -8.41747366e-01]
 [ 5.02881417e-01 -1.24528809e+00]
 [-1.05795222e+00 -9.09007615e-01]
 [ 5.51454045e-01  2.29220801e+00]
 [ 4.15393930e-02 -1.11792545e+00]
 [ 5.39058321e-01 -5.96159700e-01]
 [-1.91304965e-02  1.17500122e+00]
 [-7.47870949e-01  9.02525097e-03]
 [-8.78107893e-01 -1.56434170e-01]
 [ 2.56570452e-01 -9.88779049e-01]
 [-3.38821966e-01 -2.36184031e-01]
 [-6.37655012e-01 -1.18761229e+00]
 [-1.42121723e+00 -1.53495196e-01]
 [-2.69056960e-01  2.23136679e+00]
 [-2.43476758e+00  1.12726505e-01]
 [ 3.70444537e-01  1.35963386e+00]
 [ 5.01857207e-01 -8.44213704e-01]
 [ 9.76147160e-06  5.42352572e-01]
 [-3.13508197e-01  7.71011738e-01]
 [-1.86809065e+00  1.73118467e+00]
 [ 1.46767801e+00 -3.35677339e-01]
 [ 6.11340780e-01  4.79705919e-02]
 [-8.29135289e-01  8.77102184e-02]
 [ 1.00036589e+00 -3.81092518e-01]
 [-3.75669423e-01 -7.44707629e-02]
 [ 4.33496330e-01  1.27837923e+00]
 [-6.34679305e-01  5.08396243e-01]
 [ 2.16116006e-01 -1.85861239e+00]
 [-4.19316482e-01 -1.32328898e-01]
 [-3.95702397e-02  3.26003433e-01]
 [-2.04032305e+00  4.62555231e-02]
 [-6.77675577e-01 -1.43943903e+00]
 [ 5.24296430e-01  7.35279576e-01]
 [-6.53250268e-01  8.42456282e-01]
 [-3.81516482e-01  6.64890091e-02]
 [-1.09873895e+00  1.58448706e+00]
 [-2.65944946e+00 -9.14526229e-02]
 [ 6.95119605e-01 -2.03346655e+00]
 [-1.89469265e-01 -7.72186654e-02]
 [ 8.24703005e-01  1.24821292e+00]
 [-4.03892269e-01 -1.38451867e+00]
 [ 1.36723542e+00  1.21788563e+00]
 [-4.62005348e-01  3.50888494e-01]
 [ 3.81866234e-01  5.66275441e-01]
 [ 2.04207979e-01  1.40669624e+00]
 [-1.73795950e+00  1.04082395e+00]
 [ 3.80471970e-01 -2.17135269e-01]
 [ 1.17353150e+00 -2.34360319e+00]
 [ 1.16152149e+00  3.86078048e-01]
 [-1.13313327e+00  4.33092555e-01]
 [-3.04086439e-01  2.58529487e+00]
 [ 1.83533272e+00  4.40689872e-01]
 [-7.19253841e-01 -5.83414595e-01]
 [-3.25049628e-01 -5.60234506e-01]
 [-9.02246068e-01 -5.90972275e-01]
 [-2.76179492e-01 -5.16883894e-01]
 [-6.98589950e-01 -9.28891925e-01]
 [ 2.55043824e+00 -1.47317325e+00]
 [-1.02141473e+00  4.32395701e-01]
 [-3.23580070e-01  4.23824708e-01]
 [ 7.99179995e-01  1.26261366e+00]
 [ 7.51964849e-01 -9.93760983e-01]
 [ 1.10914328e+00 -1.76491773e+00]
 [-1.14421297e-01 -4.98174194e-01]
 [-1.06079904e+00  5.91666521e-01]
 [-1.83256574e-01  1.01985473e+00]
 [-1.48246548e+00  8.46311892e-01]
 [ 4.97940148e-01  1.26504175e-01]
 [-1.41881055e+00 -2.51774118e-01]
 [-1.54667461e+00 -2.08265194e+00]
 [ 3.27974540e+00  9.70861320e-01]
 [ 1.79259285e+00 -4.29013319e-01]
 [ 6.96197980e-01  6.97416272e-01]
 [ 6.01515814e-01  3.65949071e-03]
 [-2.28247558e-01 -2.06961226e+00]
 [ 6.10144086e-01  4.23496900e-01]
 [ 1.11788673e+00 -2.74242089e-01]
 [ 1.74181219e+00 -4.47500876e-01]
 [-1.25542722e+00  9.38163671e-01]
 [-4.68346260e-01 -1.25472031e+00]
 [ 1.24823646e-01  7.56502143e-01]
 [ 2.41439629e-01  4.97425649e-01]
 [ 4.10869262e+00  8.21120877e-01]
 [ 1.53176032e+00 -1.98584577e+00]
 [ 3.65053516e-01  7.74082033e-01]
 [-3.64479092e-01 -8.75979478e-01]
 [ 3.96520159e-01 -3.14617436e-01]
 [-5.93755583e-01  1.14950057e+00]
 [ 1.33556617e+00  3.02629336e-01]
 [-4.54227855e-01  5.14370717e-01]
 [ 8.29458431e-01  6.30621967e-01]
 [-1.45336435e+00 -3.38017777e-01]
 [ 3.59133332e-01  6.22220414e-01]
 [ 9.60781945e-01  7.58370347e-01]
 [-1.13431848e+00 -7.07420888e-01]
 [-1.22142917e+00  1.80447664e+00]
 [ 1.80409807e-01  5.53164274e-01]
 [ 1.03302907e+00 -3.29002435e-01]
 [-1.15100294e+00 -4.26522471e-01]
 [-1.48147191e-01  1.50143692e+00]
 [ 8.69598198e-01 -1.08709057e+00]
 [ 6.64221413e-01  7.34884668e-01]
 [-1.06136574e+00 -1.08516824e-01]
 [-1.85040397e+00  3.30488064e-01]
 [-3.15693210e-01 -1.35000210e+00]
 [-6.98170998e-01  2.39951198e-01]
 [-5.52949440e-01  2.99526813e-01]
 [ 5.52663696e-01 -8.40443012e-01]
 [-3.12270670e-01  2.14467809e+00]
 [ 1.21105582e-01 -8.46828752e-01]
 [ 6.04624490e-02 -1.33858888e+00]
 [ 1.13274608e+00  3.70304843e-01]
 [ 1.08580640e+00  9.02179395e-01]
 [ 3.90296450e-01  9.75509412e-01]
 [ 1.91573647e-01 -6.62209012e-01]
 [-1.02351498e+00 -4.48174823e-01]
 [-2.50545813e+00  1.82599446e+00]
 [-1.71406741e+00 -7.66395640e-02]
 [-1.31756727e+00 -2.02559359e+00]
 [-8.22453750e-02 -3.04666585e-01]
 [-1.59724130e-01  5.48946560e-01]
 [-6.18375485e-01  3.78794466e-01]
 [ 5.13251444e-01 -3.34844125e-01]
 [-2.83519516e-01  5.38424263e-01]
 [ 5.72509465e-02  1.59088487e-01]
 [-2.37440268e+00  5.85199353e-02]
 [ 3.76545911e-01 -1.35479764e-01]
 [ 3.35908395e-01  1.90437591e+00]
 [ 8.53644334e-02  6.65334278e-01]
 [-8.49995503e-01 -8.52341797e-01]
 [-4.79985112e-01 -1.01964910e+00]
 [-7.60113841e-03 -9.33830661e-01]
 [-1.74996844e-01 -1.43714343e+00]
 [-1.65220029e+00 -6.75661789e-01]
 [-1.06706712e+00 -6.52931145e-01]
 [-6.12094750e-01 -3.51262461e-01]
 [ 1.04547799e+00  1.36901602e+00]
 [ 7.25353259e-01 -3.59474459e-01]
 [ 1.49695179e+00 -1.53111111e+00]
 [-2.02336394e+00  2.67972576e-01]
 [-2.20644541e-03 -1.39291883e-01]
 [ 3.25654693e-02 -1.64056022e+00]
 [-1.15669917e+00  1.23403468e+00]
 [ 1.02818490e+00 -7.21879726e-01]
 [ 1.93315697e+00 -1.07079633e+00]
 [-5.71381608e-01  2.92432067e-01]
 [-1.19499989e+00 -4.87930544e-01]
 [-1.73071165e-01 -3.95346401e-01]
 [ 8.70840765e-01  5.92806797e-01]
 [-1.09929731e+00 -6.81530644e-01]
 [ 1.80066685e-01 -6.69310440e-02]
 [-7.87749540e-01  4.24753672e-01]
 [ 8.19885117e-01 -6.31118683e-01]
 [ 7.89059649e-01 -1.62167380e+00]
 [-1.61049926e+00  4.99939764e-01]
 [-8.34515207e-01 -9.96959687e-01]
 [-2.63388077e-01 -6.77360492e-01]
 [ 3.27067038e-01 -1.45535944e+00]
 [-3.71519124e-01  3.16096597e+00]
 [ 1.09951013e-01 -1.91352322e+00]
 [ 5.99820429e-01  5.49384465e-01]
 [ 1.38378103e+00  1.48349243e-01]
 [-6.53541444e-01  1.40883398e+00]
 [ 7.12061227e-01 -1.80071604e+00]
 [ 7.47598942e-01 -2.32897001e-01]
 [ 1.11064528e+00 -3.73338813e-01]
 [ 7.86146070e-01  1.94168696e-01]
 [ 5.86204098e-01 -2.03872918e-02]
 [-4.14408598e-01  6.73134124e-02]
 [ 6.31798924e-01  4.17592731e-01]
 [ 1.61517627e+00  4.25606211e-01]
 [ 6.35363758e-01  2.10222927e+00]
 [ 6.61264168e-02  5.35558351e-01]
 [-6.03140792e-01  4.19576292e-02]
 [ 1.64191464e+00  3.11697707e-01]
 [ 1.45116990e+00 -1.06492788e+00]
 [-1.40084545e+00  3.07525527e-01]
 [-1.36963867e+00  2.67033724e+00]
 [ 1.24845030e+00 -1.24572655e+00]
 [-1.67168774e-01 -5.76610930e-01]
 [ 4.16021749e-01 -5.78472626e-02]
 [ 9.31887358e-01  1.46833213e+00]
 [-2.21320943e-01 -1.17315562e+00]
 [ 5.62669078e-01 -1.64515057e-01]
 [ 1.14485538e+00 -1.52117687e-01]
 [ 8.29789046e-01  3.36065952e-01]
 [-1.89044051e-01 -4.49328601e-01]
 [ 7.13524448e-01  2.52973487e+00]
 [ 8.37615794e-01 -1.31682403e-01]
 [ 7.07592866e-01  1.14053878e-01]
 [-1.28089518e+00  3.09846277e-01]
 [ 1.54829069e+00 -3.15828043e-01]
 [-1.12590378e+00  4.88496666e-01]
 [ 1.83094666e+00  9.40175993e-01]
 [ 1.01871705e+00  2.30237829e+00]
 [ 1.62109298e+00  7.12683273e-01]
 [-2.08703629e-01  1.37617991e-01]
 [-1.03352168e-01  8.48350567e-01]
 [-8.83125561e-01  1.54538683e+00]
 [ 1.45840073e-01 -4.00106056e-01]
 [ 8.15206041e-01 -2.07492237e+00]
 [-8.34437391e-01 -6.57718447e-01]
 [ 8.20564332e-01 -4.89157001e-01]
 [ 1.42496703e+00 -4.46857897e-01]
 [ 5.21109431e-01 -7.08194380e-01]
 [ 1.15553059e+00 -2.54530459e-01]
 [ 5.18924924e-01 -4.92994911e-01]
 [-1.08654815e+00 -2.30917497e-01]
 [ 1.09801004e+00 -1.01787805e+00]
 [-1.52939136e+00 -3.07987737e-01]
 [ 7.80754356e-01 -1.05583964e+00]
 [-5.43883381e-01  1.84301739e-01]
 [-3.30675843e-01  2.87208202e-01]
 [ 1.18952814e+00  2.12015479e-02]
 [-6.54096803e-02  7.66115904e-01]
 [-6.16350846e-02 -9.52897152e-01]
 [-1.01446306e+00 -1.11526396e+00]
 [ 1.91260068e+00 -4.52632031e-02]
 [ 5.76909718e-01  7.17805695e-01]
 [-9.38998998e-01  6.28775807e-01]
 [-5.64493432e-01 -2.08780746e+00]
 [-2.15050132e-01 -1.07502856e+00]
 [-3.37972149e-01  3.43212732e-01]
 [ 2.28253964e+00 -4.95778848e-01]
 [-1.63962832e-01  3.71622161e-01]
 [ 1.86521520e-01 -1.58429224e-01]
 [-1.08292956e+00 -9.56625520e-01]
 [-1.83376735e-01 -1.15980690e+00]
 [-6.57768362e-01 -1.25144841e+00]
 [ 1.12448286e+00 -1.49783981e+00]
 [ 1.90201722e+00 -5.80383038e-01]
 [-1.05491567e+00 -1.18275720e+00]
 [ 7.79480054e-01  1.02659795e+00]
 [-8.48666001e-01  3.31539648e-01]
 [-1.49591353e-01 -2.42440600e-01]
 [ 1.51197175e-01  7.65069481e-01]
 [-1.91663052e+00 -2.22734129e+00]
 [ 2.06689897e-01 -7.08763560e-02]
 [ 6.84759969e-01 -1.70753905e+00]
 [-9.86569665e-01  1.54353634e+00]
 [-1.31027053e+00  3.63433972e-01]
 [-7.94872445e-01 -4.05286267e-01]
 [-1.37775793e+00  1.18604868e+00]
 [-1.90382114e+00 -1.19814038e+00]
 [-9.10065643e-01  1.17645419e+00]
 [ 2.99210670e-01  6.79267178e-01]
 [-1.76606800e-02  2.36040923e-01]
 [ 4.94035871e-01  1.54627765e+00]
 [ 2.46857508e-01 -1.46877580e+00]
 [ 1.14709994e+00  9.55569845e-02]
 [-1.10743873e+00 -1.76286141e-01]
 [-9.82755667e-01  2.08668273e+00]
 [-3.44623671e-01 -2.00207923e+00]
 [ 3.03234433e-01 -8.29874845e-01]
 [ 1.28876941e+00  1.34925462e-01]
 [-1.77860064e+00 -5.00791490e-01]
 [-1.08816157e+00 -7.57855553e-01]
 [-6.43744900e-01 -2.00878453e+00]
 [ 1.96262894e-01 -8.75896370e-01]
 [-8.93609209e-01  7.51902355e-01]
 [ 1.89693224e+00 -6.29079151e-01]
 [ 1.81208553e+00 -2.05626574e+00]
 [ 5.62704887e-01 -5.82070757e-01]
 [-7.40029749e-02 -9.86496364e-01]
 [-5.94722499e-01 -3.14811843e-01]
 [-3.46940532e-01  4.11443516e-01]
 [ 2.32639090e+00 -6.34053128e-01]
 [-1.54409962e-01 -1.74928880e+00]
 [-2.51957930e+00  1.39116243e+00]
 [-1.32934644e+00 -7.45596414e-01]
 [ 2.12608498e-02  9.10917515e-01]
 [ 3.15276082e-01  1.86620821e+00]
 [-1.82497623e-01 -1.82826634e+00]
 [ 1.38955717e-01  1.19450165e-01]
 [-8.18899200e-01 -3.32639265e-01]
 [-5.86387955e-01  1.73451634e+00]
 [-6.12751558e-01 -1.39344202e+00]
 [ 2.79433757e-01 -1.82223127e+00]
 [ 4.27017458e-01  4.06987749e-01]
 [-8.44308241e-01 -5.59820113e-01]
 [-6.00520405e-01  1.61487324e+00]
 [ 3.94953220e-01 -1.20381347e+00]
 [-1.24747243e+00 -7.75462496e-02]
 [-1.33397514e-02 -7.68323250e-01]
 [ 2.91234010e-01 -1.97330948e-01]
 [ 1.07682965e+00  4.37410232e-01]
 [-9.31978663e-02  1.35631416e-01]
 [-8.82708822e-01  8.84744194e-01]
 [ 3.83204463e-01 -4.16994149e-01]
 [ 1.17796550e-01 -5.36685309e-01]
 [ 2.48718458e+00 -4.51361054e-01]
 [ 5.18836127e-01  3.64448005e-01]
 [-7.98348729e-01  5.65779713e-03]
 [-3.20934708e-01  2.49513550e-01]
 [ 2.56308392e-01  7.67625083e-01]
 [ 7.83020087e-01 -4.07063047e-01]
 [-5.24891667e-01 -5.89808683e-01]
 [-8.62531086e-01 -1.74287290e+00]]
[[1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [0]
 [1]
 [0]
 [1]
 [1]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]]
[['red'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['blue'], ['red'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['blue'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['blue'], ['blue'], ['red'], ['blue'], ['blue'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['blue'], ['blue'], ['blue'], ['red'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['blue'], ['blue'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue'], ['red'], ['red'], ['red'], ['red'], ['red'], ['red'], ['blue']]
WARNING:tensorflow:From F:\pycharmcache\venv\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.

WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

2019-03-11 18:53:46.688476: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
After 0 steps, loss is: 9.679747
After 2000 steps, loss is: 1.630938
After 4000 steps, loss is: 0.303394
After 6000 steps, loss is: 0.150194
After 8000 steps, loss is: 0.123724
After 10000 steps, loss is: 0.109868
After 12000 steps, loss is: 0.100317
After 14000 steps, loss is: 0.094361
After 16000 steps, loss is: 0.090736
After 18000 steps, loss is: 0.088312
After 20000 steps, loss is: 0.086347
After 22000 steps, loss is: 0.084526
After 24000 steps, loss is: 0.083241
After 26000 steps, loss is: 0.082133
After 28000 steps, loss is: 0.081313
After 30000 steps, loss is: 0.080637
After 32000 steps, loss is: 0.080137
After 34000 steps, loss is: 0.079734
After 36000 steps, loss is: 0.079305
After 38000 steps, loss is: 0.078953
w1:
 [[ 0.96199024 -0.5784183  -0.51273715 -0.685939   -1.236652   -1.8548323
  -0.8054253   0.7345482  -0.72819513 -1.4073868  -0.3324695 ]
 [ 0.66455346  0.03049601 -1.043516    0.685543   -0.17345296  0.70875156
  -1.5336182   0.16442642  1.0680308   0.54613787 -0.835796  ]]
b1:
 [ 0.09366781 -0.09222969  0.32802325  0.22673199  0.509652    0.06866115
 -0.01447262 -0.34689793 -0.0726015   0.07575417  0.07043334]
w2:
 [[ 0.48809904]
 [-1.4913772 ]
 [ 0.21193159]
 [-0.6359633 ]
 [ 0.69919974]
 [-1.0305494 ]
 [-1.2364928 ]
 [-1.416965  ]
 [-0.29577518]
 [ 1.7283639 ]
 [ 1.1308775 ]]
b2:
 [0.8180861]

Process finished with exit code 0

"""
```

 æ‰§è¡Œä»£ç ï¼Œæ•ˆæœå¦‚ä¸‹ï¼š é¦–å…ˆï¼Œæ•°æ®é›†å®ç°å¯è§†åŒ–ï¼Œx0 2 + x1 2 < 2 çš„ç‚¹æ˜¾ç¤ºçº¢è‰²ï¼Œ x0 2 + x1 2 â‰¥2 çš„ç‚¹æ˜¾ç¤ºè“è‰²ï¼Œå¦‚å›¾æ‰€ç¤ºï¼š 

<img src="{{ site.url }}/assets//blog_images/ML/æœºå™¨å­¦ä¹ æ¦‚å¿µ_18.png" />

æ¥ç€ï¼Œæ‰§è¡Œæ— æ­£åˆ™åŒ–çš„è®­ç»ƒè¿‡ç¨‹ï¼ŒæŠŠçº¢è‰²çš„ç‚¹å’Œè“è‰²çš„ç‚¹åˆ†å¼€ï¼Œç”Ÿæˆæ›²çº¿å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š 

<img src="{{ site.url }}/assets//blog_images/ML/æœºå™¨å­¦ä¹ æ¦‚å¿µ_19.png" />

æœ€åï¼Œæ‰§è¡Œæœ‰æ­£åˆ™åŒ–çš„è®­ç»ƒè¿‡ç¨‹ï¼ŒæŠŠçº¢è‰²çš„ç‚¹å’Œè“è‰²çš„ç‚¹åˆ†å¼€ï¼Œç”Ÿæˆæ›²çº¿å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š 

<img src="{{ site.url }}/assets//blog_images/ML/æœºå™¨å­¦ä¹ æ¦‚å¿µ_20.png" />

å¯¹æ¯”æ— æ­£åˆ™åŒ–ä¸æœ‰æ­£åˆ™åŒ–æ¨¡å‹çš„è®­ç»ƒç»“æœï¼Œå¯çœ‹å‡ºæœ‰æ­£åˆ™åŒ–æ¨¡å‹çš„æ‹Ÿåˆæ›²çº¿å¹³æ»‘ï¼Œæ¨¡å‹å…·æœ‰æ›´å¥½çš„æ³› åŒ–èƒ½åŠ›ã€‚ 

 **æ­å»ºæ¨¡å—åŒ–ç¥ç»ç½‘ç»œå…«è‚¡**

 âˆš**å‰å‘ä¼ æ’­ï¼šç”±è¾“å…¥åˆ°è¾“å‡ºï¼Œæ­å»ºå®Œæ•´çš„ç½‘ç»œç»“æ„** 

æè¿°å‰å‘ä¼ æ’­çš„è¿‡ç¨‹éœ€è¦å®šä¹‰ä¸‰ä¸ªå‡½æ•°ï¼š

 

```python
âˆšdef forward(x, regularizer): 

    w= 

     b= 

    y= 

    return y 
```

ç¬¬ä¸€ä¸ªå‡½æ•° forward()å®Œæˆç½‘ç»œç»“æ„çš„è®¾è®¡ï¼Œä»è¾“å…¥åˆ°è¾“å‡ºæ­å»ºå®Œæ•´çš„ç½‘ç»œç»“æ„ï¼Œå®ç°å‰å‘ä¼ æ’­è¿‡ç¨‹ã€‚ è¯¥å‡½æ•°ä¸­ï¼Œå‚æ•° x ä¸ºè¾“å…¥ï¼Œregularizer ä¸ºæ­£åˆ™åŒ–æƒé‡ï¼Œè¿”å›å€¼ä¸ºé¢„æµ‹æˆ–åˆ†ç±»ç»“æœ yã€‚ 

```
âˆšdef get_weight(shape, regularizer): 

	w = tf.Variable( )

    tf.add_to_collection('losses', 		                 (ç©ºæ ¼)tf.contrib.layers.l2_regularizer(regularizer)(w))

 return w 
```

ç¬¬äºŒä¸ªå‡½æ•° get_weight()å¯¹å‚æ•° w è®¾å®šã€‚è¯¥å‡½æ•°ä¸­ï¼Œå‚æ•° shape è¡¨ç¤ºå‚æ•° w çš„å½¢çŠ¶ï¼Œregularizer è¡¨ç¤ºæ­£åˆ™åŒ–æƒé‡ï¼Œè¿”å›å€¼ä¸ºå‚æ•° wã€‚å…¶ä¸­ï¼Œtf.variable()ç»™ w èµ‹åˆå€¼ï¼Œtf.add_to_collection()è¡¨ ç¤ºå°†å‚æ•° w æ­£åˆ™åŒ–æŸå¤±åŠ åˆ°æ€»æŸå¤± losses ä¸­ã€‚

 

```python
âˆšdef get_bias(shape): 

b = tf.Variable( ) 

return b
```

 

ç¬¬ä¸‰ä¸ªå‡½æ•° get_bias()å¯¹å‚æ•° b è¿›è¡Œè®¾å®šã€‚è¯¥å‡½æ•°ä¸­ï¼Œå‚æ•° shape è¡¨ç¤ºå‚æ•° b çš„å½¢çŠ¶,è¿”å›å€¼ä¸ºå‚æ•° bã€‚å…¶ä¸­ï¼Œtf.variable()è¡¨ç¤ºç»™ b èµ‹åˆå€¼ã€‚

 âˆšåå‘ä¼ æ’­ï¼šè®­ç»ƒç½‘ç»œï¼Œä¼˜åŒ–ç½‘ç»œå‚æ•°ï¼Œæé«˜æ¨¡å‹å‡†ç¡®æ€§ã€‚ 

```python
âˆšdef backward( ):

 x = tf.placeholder( )

 y_ = tf.placeholder( ) 

y = forward.forward(x, REGULARIZER) 

global_step = tf.Variable(0, trainable=False) 

loss =
```

 å‡½æ•° backward()ä¸­ï¼Œplaceholder()å®ç°å¯¹æ•°æ®é›† x å’Œæ ‡å‡†ç­”æ¡ˆ y_å ä½ï¼Œforward.forward()å®ç°å‰å‘ ä¼ æ’­çš„ç½‘ç»œç»“æ„ï¼Œå‚æ•° global_step è¡¨ç¤ºè®­ç»ƒè½®æ•°ï¼Œè®¾ç½®ä¸ºä¸å¯è®­ç»ƒå‹å‚æ•°ã€‚

 åœ¨è®­ç»ƒç½‘ç»œæ¨¡å‹æ—¶ï¼Œå¸¸å°†æ­£åˆ™åŒ–ã€æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡å’Œæ»‘åŠ¨å¹³å‡è¿™ä¸‰ä¸ªæ–¹æ³•ä½œä¸ºæ¨¡å‹ä¼˜åŒ–æ–¹æ³•ã€‚

 âˆšåœ¨ Tensorflow ä¸­ï¼Œæ­£åˆ™åŒ–è¡¨ç¤ºä¸ºï¼š

 é¦–å…ˆï¼Œè®¡ç®—é¢„æµ‹ç»“æœä¸æ ‡å‡†ç­”æ¡ˆçš„æŸå¤±å€¼

 â‘ MSEï¼š y ä¸ y_çš„å·®è·(loss_mse) = tf.reduce_mean(tf.square(y-y_)) 

â‘¡äº¤å‰ç†µï¼šce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))_

 y ä¸ y__çš„å·®è·(cem) = tf.reduce_mean(ce)_

_â‘¢è‡ªå®šä¹‰ï¼šy ä¸ y__çš„å·®è· 

å…¶æ¬¡ï¼Œæ€»æŸå¤±å€¼ä¸ºé¢„æµ‹ç»“æœä¸æ ‡å‡†ç­”æ¡ˆçš„æŸå¤±å€¼åŠ ä¸Šæ­£åˆ™åŒ–é¡¹

 loss = y ä¸ y_çš„å·®è· + tf.add_n(tf.get_collection('losses')) _

_âˆšåœ¨ Tensorflow ä¸­ï¼ŒæŒ‡æ•°è¡°å‡å­¦ä¹ ç‡è¡¨ç¤ºä¸ºï¼š

 learning_rate = tf.train.exponential_decay(

 LEARNING_RATE_BASE,

 global_step, 

æ•°æ®é›†æ€»æ ·æœ¬æ•° / BATCH_SIZE,

 LEARNING_RATE_DECAY, 

staircase=True) train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)

 âˆšåœ¨ Tensorflow ä¸­ï¼Œæ»‘åŠ¨å¹³å‡è¡¨ç¤ºä¸ºï¼š

 ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step) ema_op = ema.apply(tf.trainable_variables())

 with tf.control_dependencies([train_step, ema_op]): 

train_op = tf.no_op(name='train') 

å…¶ä¸­ï¼Œæ»‘åŠ¨å¹³å‡å’ŒæŒ‡æ•°è¡°å‡å­¦ä¹ ç‡ä¸­çš„ global_step ä¸ºåŒä¸€ä¸ªå‚æ•°ã€‚ 

âˆšç”¨ with ç»“æ„åˆå§‹åŒ–æ‰€æœ‰å‚æ•° 

```python
with tf.Session() as sess:

    init_op = tf.global_variables_initializer()

    sess.run(init_op)

 for i in range(STEPS):

       sess.run(train_step, feed_dict={x: , y_: })_

               if i % è½®æ•° == 0:  

               print 
```

å…¶ä¸­ï¼Œwith ç»“æ„ç”¨äºåˆå§‹åŒ–æ‰€æœ‰å‚æ•°ä¿¡æ¯ä»¥åŠå®ç°è°ƒç”¨è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶æ‰“å°å‡º loss å€¼ã€‚ âˆšåˆ¤æ–­ python è¿è¡Œæ–‡ä»¶æ˜¯å¦ä¸ºä¸»æ–‡ä»¶

```python
 if __name__=='__main__': 

backward()
```

 è¯¥éƒ¨åˆ†ç”¨æ¥åˆ¤æ–­ python è¿è¡Œçš„æ–‡ä»¶æ˜¯å¦ä¸ºä¸»æ–‡ä»¶ã€‚è‹¥æ˜¯ä¸»æ–‡ä»¶ï¼Œåˆ™æ‰§è¡Œ backword()å‡½æ•°ã€‚

 ä¾‹å¦‚ï¼š ç”¨ 300 ä¸ªç¬¦åˆæ­£æ€åˆ†å¸ƒçš„ç‚¹ X[x0, x1]ä½œä¸ºæ•°æ®é›†ï¼Œæ ¹æ®ç‚¹ X[x0, x1]çš„ä¸åŒè¿›è¡Œæ ‡æ³¨ Y_ï¼Œå°†æ•°æ®é›†æ ‡ æ³¨ä¸ºçº¢è‰²å’Œè“è‰²ã€‚æ ‡æ³¨è§„åˆ™ä¸ºï¼šå½“ x0 2 + x1 2 < 2 æ—¶ï¼Œy_=1ï¼Œç‚¹ X æ ‡æ³¨ä¸ºçº¢è‰²ï¼›å½“ x0 2 + x1 2 â‰¥2 æ—¶ï¼Œ y_=0ï¼Œç‚¹ X æ ‡æ³¨ä¸ºè“è‰²ã€‚æˆ‘ä»¬åŠ å…¥æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡ä¼˜åŒ–æ•ˆç‡ï¼ŒåŠ å…¥æ­£åˆ™åŒ–æé«˜æ³›åŒ–æ€§ï¼Œå¹¶ä½¿ç”¨æ¨¡å—åŒ– è®¾è®¡æ–¹æ³•ï¼ŒæŠŠçº¢è‰²ç‚¹å’Œè“è‰²ç‚¹åˆ†å¼€ã€‚ 

ä»£ç æ€»å…±åˆ†ä¸ºä¸‰ä¸ªæ¨¡å—ï¼šç”Ÿæˆæ•°æ®é›† (generateds.py)ã€å‰å‘ä¼ æ’­(forward.py)ã€åå‘ä¼ æ’­ (backward.py)ã€‚

 â‘ ç”Ÿæˆæ•°æ®é›†çš„æ¨¡å—(generateds.py) 

â‘¡å‰å‘ä¼ æ’­æ¨¡å—(forward.py) 

<img src="{{ site.url }}/assets//blog_images/ML/æœºå™¨å­¦ä¹ æ¦‚å¿µ_21.png" />

â‘¢åå‘ä¼ æ’­æ¨¡å—(backward.py) 

<img src="{{ site.url }}/assets//blog_images/ML/æœºå™¨å­¦ä¹ æ¦‚å¿µ_22.png" />

<img src="{{ site.url }}/assets//blog_images/ML/æœºå™¨å­¦ä¹ æ¦‚å¿µ_23.png" />

è¿è¡Œä»£ç ï¼Œç»“æœå¦‚ä¸‹ï¼š 

<img src="{{ site.url }}/assets//blog_images/ML/æœºå™¨å­¦ä¹ æ¦‚å¿µ_24.png" />

ç”±è¿è¡Œç»“æœå¯è§ï¼Œç¨‹åºä½¿ç”¨æ¨¡å—åŒ–è®¾è®¡æ–¹æ³•ï¼ŒåŠ å…¥æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡ï¼Œä½¿ç”¨æ­£åˆ™åŒ–åï¼Œçº¢è‰²ç‚¹å’Œè“è‰²ç‚¹ çš„åˆ†å‰²æ›²çº¿ç›¸å¯¹å¹³æ»‘ï¼Œæ•ˆæœå˜å¥½ã€‚ 



















